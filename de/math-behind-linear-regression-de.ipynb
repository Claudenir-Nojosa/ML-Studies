{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d926547c",
   "metadata": {},
   "source": [
    "# Lineare Regression\n",
    "\n",
    "## Grundlegendes Konzept\n",
    "\n",
    "Die lineare Regression ist ein maschineller Lernalgorithmus, dessen Hauptziel es ist, die beste Gerade zu finden, die durch die verfügbaren Datenpunkte verläuft. Diese Methode versucht, eine lineare Beziehung zwischen Eingabe- und Ausgabevariablen herzustellen und ermöglicht präzise Vorhersagen für neue Daten.\n",
    "\n",
    "## Überwachtes Lernen\n",
    "\n",
    "![Titelbild](../Assets/linear_regression/input.png)\n",
    "\n",
    "Die lineare Regression wird als **überwachter Lernalgorithmus** (*supervised learning*) klassifiziert. Diese Charakterisierung ergibt sich aus der Tatsache, dass der Algorithmus einen Trainingsdatensatz verwendet, der sowohl Eingabe-*Features* als auch entsprechende Ausgabe-*Targets* enthält. Während des Trainingsprozesses lernt das Modell, die Beziehungen zwischen diesen bekannten Eingaben und Ausgaben abzubilden.\n",
    "\n",
    "## Mathematisches Modell\n",
    "\n",
    "Sobald das Modell trainiert ist, wird es fähig, Werte $\\hat{y}$ (*y-hat*) für neue Eingaben $x$ vorherzusagen. Diese Vorhersage kann mathematisch durch folgende Funktion dargestellt werden:\n",
    "\n",
    "![Titelbild](../Assets/linear_regression/w_e_b.png)\n",
    "\n",
    "$$f(x) = wx + b$$\n",
    "\n",
    "Dabei:\n",
    "- $f(x)$ oder $\\hat{y}$ stellt den vorhergesagten Wert dar\n",
    "- $w$ ist die Steigung der Geraden\n",
    "- $b$ ist der Achsenabschnitt der Geraden\n",
    "- $x$ ist das Eingabe-Feature\n",
    "\n",
    "## Modellparameter\n",
    "\n",
    "Die für die Parameter $w$ und $b$ gewählten Werte sind grundlegend, da sie das Verhalten des Modells vollständig bestimmen. Insbesondere definieren diese Parameter den Vorhersagewert $\\hat{y}_i$ für jedes Beispiel $i$, basierend auf dem entsprechenden Eingabe-Feature $x_i$. Die Optimierung dieser Parameter während des Trainings ermöglicht es dem Modell, präzise Vorhersagen zu treffen.\n",
    "\n",
    "## Kostenfunktion\n",
    "\n",
    "Um die besten Werte für $w$ und $b$ zu finden, reduzieren wir die sogenannte **Kostenfunktion** (*cost function*). Das Ziel ist es, Werte für $w$ und $b$ zu wählen, sodass die Vorhersage $\\hat{y}_i$ dem tatsächlichen Wert $y_i$ für alle Beispiele im Trainingssatz so nahe wie möglich kommt.\n",
    "\n",
    "### Aufbau der Kostenfunktion\n",
    "\n",
    "Die Kostenfunktion funktioniert, indem sie den tatsächlichen Wert $y_i$ mit dem vorhergesagten Wert $\\hat{y}_i$ vergleicht. Die Differenz zwischen diesen Werten wird ausgedrückt als:\n",
    "\n",
    "$$\\hat{y}_i - y_i$$\n",
    "\n",
    "Diese Differenz wird als **Fehler** (*error*) der Vorhersage bezeichnet.\n",
    "\n",
    "Um eine robustere Metrik zu erhalten, quadrieren wir diesen Fehler. Dies dient zwei wichtigen Zwecken: Eliminierung negativer Werte (sodass sich positive und negative Fehler nicht gegenseitig aufheben) und intensivere Bestrafung größerer Fehler:\n",
    "\n",
    "$$(\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "Da wir den Fehler über den gesamten Datensatz quantifizieren möchten, summieren wir die quadrierten Fehler aller Trainingsbeispiele:\n",
    "\n",
    "$$\\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "wobei $m$ die Gesamtzahl der Trainingsbeispiele oder Datenpunkte darstellt.\n",
    "\n",
    "Schließlich berechnen wir den Durchschnitt dieser Fehler, indem wir durch $2m$ teilen. Die Wahl von $2m$ anstelle von nur $m$ soll die Mathematik später vereinfachen, insbesondere bei der Berechnung der partiellen Ableitung der Kostenfunktion.\n",
    "\n",
    "Da $\\hat{y}_i$ als $f(x_i)$ dargestellt werden kann, wird die vollständige **Kostenfunktion** ausgedrückt als:\n",
    "\n",
    "$$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x_i) - y_i)^2$$\n",
    "\n",
    "Diese Funktion, bekannt als **mittlerer quadratischer Fehler** (*Mean Squared Error* - MSE), ist die am häufigsten verwendete Kostenfunktion bei Problemen der linearen Regression.\n",
    "\n",
    "## Gradientenabstieg\n",
    "\n",
    "Um die Kostenfunktion zu minimieren, verwenden wir einen Algorithmus namens **Gradientenabstieg** (*Gradient Descent*). Der Prozess ist relativ einfach: Wir beginnen mit Anfangswerten für $w$ und $b$ (üblicherweise $w = 0$ und $b = 0$).\n",
    "\n",
    "Anschließend aktualisieren wir die Parameter $w$ und $b$ wiederholt in kleinen Schritten mit dem Ziel, die Kosten zu reduzieren. Wir setzen diesen iterativen Prozess fort, bis wir die niedrigsten möglichen Kosten (lokales Minimum) erreichen. Wenn der Algorithmus diesen Punkt erreicht, sagen wir, er ist **konvergiert**.\n",
    "\n",
    "![Gradientenabstieg](../Assets/linear_regression/gradient.png)\n",
    "\n",
    "Der Prozess kann mit dem Abstieg eines Berges verglichen werden: Jeder Schritt bringt uns näher zum Boden des Tals. Die Richtung in jedem Schritt wird durch den **Gradienten** bestimmt, der immer in Richtung des steilsten Anstiegs zeigt.\n",
    "\n",
    "![Gradientenabstieg Schritte](../Assets/linear_regression/steps.png)\n",
    "\n",
    "Um die Kosten zu minimieren, bewegen wir uns in die entgegengesetzte Richtung des Gradienten, das heißt, wir machen Schritte bergab.\n",
    "\n",
    "### Gradientenabstiegs-Formel\n",
    "\n",
    "Die Parameteraktualisierung bei jeder Iteration folgt diesen Gleichungen:\n",
    "\n",
    "$w = w - \\alpha \\frac{\\partial J(w,b)}{\\partial w}$\n",
    "\n",
    "$b = b - \\alpha \\frac{\\partial J(w,b)}{\\partial b}$\n",
    "\n",
    "Dabei:\n",
    "- $w$ und $b$ werden bei jeder Iteration simultan aktualisiert\n",
    "- $\\alpha$ ist die **Lernrate** (*learning rate*)\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ und $\\frac{\\partial J(w,b)}{\\partial b}$ sind die partiellen Ableitungen der Kostenfunktion\n",
    "\n",
    "### Lernrate\n",
    "\n",
    "Die Wahl eines guten Wertes für $\\alpha$ ist entscheidend:\n",
    "\n",
    "- **$\\alpha$ zu klein**: Der Algorithmus macht sehr kurze Schritte, was die Konvergenz langsam und zeitaufwendig macht\n",
    "- **$\\alpha$ zu groß**: Der Algorithmus kann über das Minimum \"springen\", konvergiert nicht oder divergiert sogar\n",
    "\n",
    "![Lernrate](../Assets/linear_regression/alpha.png)\n",
    "\n",
    "## Berechnung der partiellen Ableitungen\n",
    "\n",
    "Um den Gradientenabstieg zu implementieren, müssen wir die partiellen Ableitungen der Kostenfunktion $J(w,b)$ in Bezug auf $w$ und $b$ berechnen. Wir beginnen mit unserer Kostenfunktion:\n",
    "\n",
    "$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x_i) - y_i)^2$\n",
    "\n",
    "Da $f(x_i) = wx_i + b$, können wir umschreiben:\n",
    "\n",
    "$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} ((wx_i + b) - y_i)^2$\n",
    "\n",
    "### Partielle Ableitung nach $w$\n",
    "\n",
    "Anwendung der Kettenregel zur Ableitung von $J(w,b)$ nach $w$:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[\\frac{1}{2m} \\sum_{i=1}^{m} ((wx_i + b) - y_i)^2\\right]$\n",
    "\n",
    "Die Konstante $\\frac{1}{2m}$ kann aus der Ableitung herausgezogen werden:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{2m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial w} ((wx_i + b) - y_i)^2$\n",
    "\n",
    "Anwendung der Kettenregel: $\\frac{d}{dx}[g(x)]^2 = 2g(x) \\cdot g'(x)$\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{2m} \\sum_{i=1}^{m} 2((wx_i + b) - y_i) \\cdot \\frac{\\partial}{\\partial w}((wx_i + b) - y_i)$\n",
    "\n",
    "Da $\\frac{\\partial}{\\partial w}((wx_i + b) - y_i) = x_i$:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{2m} \\sum_{i=1}^{m} 2((wx_i + b) - y_i) \\cdot x_i$\n",
    "\n",
    "Der Faktor $2$ hebt sich mit der $2$ im Nenner auf:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} x_i \\big((w x_i + b) - y_i\\big)$\n",
    "\n",
    "Ersetzen von $wx_i + b$ durch $f(x_i)$:\n",
    "\n",
    "$\\boxed{\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} x_i(f(x_i) - y_i) }$\n",
    "\n",
    "### Partielle Ableitung nach $b$\n",
    "\n",
    "Nach demselben Verfahren für $b$:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[\\frac{1}{2m} \\sum_{i=1}^{m} ((wx_i + b) - y_i)^2\\right]$\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{2m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial b} ((wx_i + b) - y_i)^2$\n",
    "\n",
    "Anwendung der Kettenregel:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{2m} \\sum_{i=1}^{m} 2((wx_i + b) - y_i) \\cdot \\frac{\\partial}{\\partial b}((wx_i + b) - y_i)$\n",
    "\n",
    "Da $\\frac{\\partial}{\\partial b}((wx_i + b) - y_i) = 1$:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{2m} \\sum_{i=1}^{m} 2((wx_i + b) - y_i) \\cdot 1$\n",
    "\n",
    "Vereinfachung:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} ((wx_i + b) - y_i)$\n",
    "\n",
    "Ersetzen von $(wx_i + b)$ durch $f(x_i)$:\n",
    "\n",
    "$\\boxed{\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x_i) - y_i)}$\n",
    "\n",
    "### Vollständiger Algorithmus\n",
    "\n",
    "Mit den berechneten Ableitungen lautet der Gradientenabstiegs-Algorithmus für lineare Regression:\n",
    "\n",
    "**Wiederholen bis zur Konvergenz:**\n",
    "\n",
    "$w = w - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} x_i(f(x_i) - y_i) $\n",
    "\n",
    "$b = b - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (f(x_i) - y_i)$\n",
    "\n",
    "Dabei $f(x_i) = wx_i + b$\n",
    "\n",
    "> **Wichtiger Hinweis**: Die Parameter $w$ und $b$ müssen bei jeder Iteration **simultan** aktualisiert werden, das heißt, wir berechnen beide Ableitungen mit den alten Werten, bevor wir einen Parameter aktualisieren.\n",
    "\n",
    "# Unten finden Sie ein Beispiel mit Python-Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6052bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren der Bibliotheken\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af232dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importieren unseres Datensatzes\n",
    "training_set = pd.read_csv('../Datasets/Salary_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition unseres Parameters und Ziels\n",
    "X_train = training_set['YearsExperience'].values\n",
    "y_train = training_set['Salary'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter-Darstellung des Parameters mit dem Ziel\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir brauchen drei Hauptfunktionen, um die lineare Regression zu implementieren:\n",
    "# 1) Kostenfunktion: Berechnet, wie gut das Modell ist, mit dem Mean Squared Error (MSE), der den durchschnittlichen quadrierten Fehler zwischen den vorhergesagten Werten und den echten Werten misst.\n",
    "\n",
    "# 2) Gradientenfunktion: Berechnet die Ableitungen der Kostenfunktion in Bezug auf die Parameter w und b.\n",
    "\n",
    "# 3) Gradientenabstiegsfunktion: Verwendet die berechneten Gradienten, um die Parameter w und b in jeder Iteration zu aktualisieren, mit dem Ziel, den Fehler zu minimieren.\n",
    "\n",
    "def cost_function(x, y, w, b):\n",
    "    m = len(x)\n",
    "    cost_sum = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f = w * x[i] + b\n",
    "        cost = (f - y[i]) ** 2\n",
    "        cost_sum += cost\n",
    "\n",
    "    total_cost = (1/(2*m)) * cost_sum\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def gradient_function(x, y, w, b):\n",
    "    m = len(x)\n",
    "    dc_dw = 0\n",
    "    dc_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f = w * x[i] + b\n",
    "\n",
    "        dc_dw += (f - y[i]) * x[i]\n",
    "        dc_db += (f - y[i])\n",
    "\n",
    "    dc_dw = (1/m) * dc_dw\n",
    "    dc_db = (1/m) * dc_db\n",
    "\n",
    "    return dc_dw, dc_db\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, alpha, iterations):\n",
    "    w = 0\n",
    "    b = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        dc_dw, dc_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        w = w - alpha * dc_dw\n",
    "        b = b - alpha * dc_db\n",
    "\n",
    "        print(f\"Iteration {i}: Cost {cost_function(x, y, w, b)}\")\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gibt die Lernrate und die Anzahl der Iterationen an\n",
    "learning_rate = 0.01\n",
    "iterations = 10000\n",
    "# Berechnet den Gradientenabstieg\n",
    "final_w, final_b = gradient_descent(\n",
    "    X_train, y_train, learning_rate, iterations)\n",
    "print(f\"w: {final_w:.4f}, b: {final_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db6a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisiert die Regressionslinie\n",
    "plt.scatter(X_train, y_train, label='Data Points')\n",
    "\n",
    "X_vals = np.linspace(min(X_train), max(X_train), 100)\n",
    "y_vals = final_w * X_vals + final_b\n",
    "plt.plot(X_vals, y_vals, color='red', label='Regression Line')\n",
    "\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f69663",
   "metadata": {},
   "source": [
    "# Optimierung der Linearen Regression\n",
    "\n",
    "## Einführung\n",
    "\n",
    "Nach der Implementierung des grundlegenden linearen Regressionsalgorithmus mit Gradientenabstieg treten in der Praxis zwei häufige Herausforderungen auf:\n",
    "\n",
    "1. **Sehr hohe Kostenwerte** - Erschwerung der Interpretation und Konvergenz\n",
    "2. **Ungeeignete Wahl der Lernrate** - Führt zu langsamer Konvergenz oder Trainingsversagen\n",
    "\n",
    "Dieser Leitfaden präsentiert zwei wesentliche Techniken zur Lösung dieser Probleme: **Merkmalsnormalisierung** und **systematisches Testen von Lernraten**.\n",
    "\n",
    "---\n",
    "\n",
    "## Merkmalsnormalisierung\n",
    "\n",
    "### Das Skalenproblem\n",
    "\n",
    "Beim Arbeiten mit Daten auf verschiedenen Skalen stößt der Gradientenabstiegs-Algorithmus auf Schwierigkeiten. Wenn beispielsweise ein Merkmal zwischen 0 und 100 variiert und ein anderes zwischen 0 und 100.000, haben die Gradienten sehr unterschiedliche Größenordnungen, was verursacht:\n",
    "\n",
    "- **Langsame Konvergenz**: Der Algorithmus benötigt viele Iterationen\n",
    "- **Numerische Instabilität**: Extrem große Kostenwerte\n",
    "- **Schwierigkeit bei der Wahl der Lernrate**: Ein α, das für ein Merkmal funktioniert, kann für ein anderes ungeeignet sein\n",
    "\n",
    "### Lösung: Z-Score-Normalisierung\n",
    "\n",
    "Die **Z-Score**-Normalisierung transformiert die Daten so, dass sie einen Mittelwert von 0 und eine Standardabweichung von 1 haben:\n",
    "\n",
    "$$X_{norm} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Dabei:\n",
    "- $\\mu$ ist der Mittelwert der Daten\n",
    "- $\\sigma$ ist die Standardabweichung der Daten\n",
    "\n",
    "### Implementierung\n",
    "\n",
    "```python\n",
    "def normalize_features(X):\n",
    "    \"\"\"Normalisiert Daten mittels Z-Score\"\"\"\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X_norm = (X - mean) / std\n",
    "    return X_norm\n",
    "```\n",
    "\n",
    "**Parameter:**\n",
    "- `X`: Array mit Originaldaten\n",
    "\n",
    "**Rückgabe:**\n",
    "- `X_norm`: Normalisierte Daten\n",
    "\n",
    "### Vorteile der Normalisierung\n",
    "\n",
    "1. **Drastische Kostenreduzierung**: Von Millionen auf Werte nahe 0\n",
    "2. **Schnellere Konvergenz**: Weniger Iterationen erforderlich\n",
    "3. **Ausgeglichene Gradienten**: Alle Merkmale tragen gleichermaßen bei\n",
    "4. **Erleichtert die Wahl der Lernrate**: Typische Werte (0,01 bis 1,0) funktionieren gut\n",
    "\n",
    "### Praktisches Beispiel\n",
    "\n",
    "Vor der Normalisierung:\n",
    "```\n",
    "X: min=1.10, max=10.50, mean=5.31\n",
    "y: min=$37,731, max=$122,391, mean=$76,003\n",
    "Anfangskosten: 1,344,612,525\n",
    "```\n",
    "\n",
    "Nach der Normalisierung:\n",
    "```\n",
    "X_norm: min=-1.51, max=1.86, mean=0.00\n",
    "y_norm: min=-1.42, max=1.72, mean=0.00\n",
    "Anfangskosten: 0.499\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Optimierung der Lernrate\n",
    "\n",
    "### Das Lernraten-Dilemma\n",
    "\n",
    "Die Lernrate ($\\alpha$) kontrolliert die Größe der Schritte, die der Algorithmus in Richtung des Minimums macht. Die Wahl dieses Wertes ist kritisch:\n",
    "\n",
    "| Lernrate | Verhalten | Ergebnis |\n",
    "|----------|-----------|----------|\n",
    "| **Zu klein** | Winzige Schritte | Sehr langsame Konvergenz |\n",
    "| **Angemessen** | Ausgeglichene Schritte | Effiziente Konvergenz |\n",
    "| **Zu groß** | Übermäßige Schritte | Oszillation oder Divergenz |\n",
    "\n",
    "### Systematische Teststrategie\n",
    "\n",
    "Anstatt willkürlich zu wählen, testen wir mehrere Werte und wählen den besten basierend auf Leistungsmetriken aus.\n",
    "\n",
    "### Implementierung\n",
    "\n",
    "```python\n",
    "def test_learning_rates(X, y):\n",
    "    \"\"\"Testet verschiedene Lernraten, um die beste zu finden\"\"\"\n",
    "    learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    iterations = 5000\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTEN VERSCHIEDENER LERNRATEN\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\n--- Teste α = {lr} ---\")\n",
    "        w, b, history = gradient_descent(\n",
    "            X, y, lr, iterations, print_every=1000)\n",
    "\n",
    "        # Berechne R²\n",
    "        predictions = w * X + b\n",
    "        ss_res = np.sum((y - predictions) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'final_cost': history[-1],\n",
    "            'r2': r2,\n",
    "            'w': w,\n",
    "            'b': b\n",
    "        })\n",
    "\n",
    "        print(f\"Endkosten: {history[-1]:.6f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Wähle das beste Ergebnis\n",
    "    best = max(results, key=lambda x: x['r2'])\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"BESTE LERNRATE: α = {best['lr']}\")\n",
    "    print(f\"R² = {best['r2']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return best['lr']\n",
    "```\n",
    "\n",
    "### Code-Aufschlüsselung\n",
    "\n",
    "#### 1. Definition der Kandidaten\n",
    "\n",
    "```python\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "```\n",
    "\n",
    "Wir testen Werte auf einer **logarithmischen Skala**, die von sehr konservativen bis zu aggressiven Werten reicht.\n",
    "\n",
    "#### 2. Training mit jedem Kandidaten\n",
    "\n",
    "```python\n",
    "for lr in learning_rates:\n",
    "    w, b, history = gradient_descent(X, y, lr, iterations, print_every=1000)\n",
    "```\n",
    "\n",
    "Jede Lernrate wird mit der gleichen Anzahl von Iterationen für einen fairen Vergleich getestet.\n",
    "\n",
    "#### 3. R²-Score-Berechnung\n",
    "\n",
    "Das **Bestimmtheitsmaß** ($R^2$) misst, wie gut das Modell die Datenvariabilität erklärt:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "Dabei:\n",
    "- $SS_{res} = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$ (Summe der quadrierten Residuen)\n",
    "- $SS_{tot} = \\sum_{i=1}^{m} (y_i - \\bar{y})^2$ (Gesamtsumme der Quadrate)\n",
    "\n",
    "```python\n",
    "predictions = w * X + b\n",
    "ss_res = np.sum((y - predictions) ** 2)\n",
    "ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "```\n",
    "\n",
    "#### R²-Interpretation:\n",
    "\n",
    "- R² = 1,0: Perfektes Modell (erklärt 100% der Varianz)\n",
    "- R² = 0,95: Ausgezeichnet (erklärt 95% der Varianz)\n",
    "- R² = 0,70: Gut (erklärt 70% der Varianz)\n",
    "- R² = 0,50: Durchschnittlich (erklärt 50% der Varianz)\n",
    "- R² < 0,30: Schlecht (Modell hat wenig Vorhersagekraft)\n",
    "- R² < 0: Modell schlechter als einfach den Mittelwert zu verwenden\n",
    "\n",
    "#### 4. Speicherung der Ergebnisse\n",
    "\n",
    "```python\n",
    "results.append({\n",
    "    'lr': lr,\n",
    "    'final_cost': history[-1],\n",
    "    'r2': r2,\n",
    "    'w': w,\n",
    "    'b': b\n",
    "})\n",
    "```\n",
    "\n",
    "Jeder Test wird in einem Wörterbuch gespeichert, das alle relevanten Metriken enthält.\n",
    "\n",
    "#### 5. Auswahl der besten Lernrate\n",
    "\n",
    "```python\n",
    "best = max(results, key=lambda x: x['r2'])\n",
    "```\n",
    "\n",
    "Wir verwenden R² als Auswahlkriterium und wählen die Lernrate, die diese Metrik maximiert.\n",
    "\n",
    "# Code in der Praxis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus-Code - Normalisierung und passende Lernrate\n",
    "\n",
    "def normalize_features(X):\n",
    "    \"\"\"Normalisiert Daten mittels Z-Score\"\"\"\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X_norm = (X - mean) / std\n",
    "    return X_norm, mean, std\n",
    "\n",
    "\n",
    "def test_learning_rates(X, y):\n",
    "    \"\"\"Testet verschiedene Lernraten, um die beste zu finden\"\"\"\n",
    "    learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    iterations = 5000\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTEN VERSCHIEDENER LERNRATEN\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\n--- Teste α = {lr} ---\")\n",
    "        w, b = gradient_descent(\n",
    "            X, y, lr, iterations)\n",
    "\n",
    "        # R²\n",
    "        predictions = w * X + b\n",
    "        ss_res = np.sum((y - predictions) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'r2': r2,\n",
    "            'w': w,\n",
    "            'b': b\n",
    "        })\n",
    "\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    # Bestes Ergebnis\n",
    "    best = max(results, key=lambda x: x['r2'])\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"BESTE LERNRATE: α = {best['lr']}\")\n",
    "    print(f\"R² = {best['r2']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return best['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisiere die Daten\n",
    "X_norm, x_mean, x_std = normalize_features(X_train)\n",
    "y_norm, y_mean, y_std = normalize_features(y_train)\n",
    "\n",
    "print(\"\\nNORMALISIERTE DATEN\")\n",
    "print(\n",
    "    f\"X_norm: min={X_norm.min():.2f}, max={X_norm.max():.2f}, mean={X_norm.mean():.2f}\")\n",
    "print(\n",
    "    f\"y_norm: min={y_norm.min():.2f}, max={y_norm.max():.2f}, mean={y_norm.mean():.2f}\")\n",
    "\n",
    "# Teste verschiedene Lernraten\n",
    "best_lr = test_learning_rates(X_norm, y_norm)\n",
    "\n",
    "# Trainiere erneut, aber jetzt mit der besten Lernrate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"ABSCHLUSSTRAINING MIT α = {best_lr}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "w_initial = 0\n",
    "b_initial = 0\n",
    "iterations = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Führe Gradientenabstieg jetzt mit viel niedrigeren Kosten durch (besser)\n",
    "w_final, b_final = gradient_descent(\n",
    "    X_norm, y_norm, best_lr, iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20979985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Visualisierung der Regressionslinie\n",
    "def plot_regression(X, y, w, b):\n",
    "    \"\"\"Plottet Daten und Regressionslinie - SEHR EINFACH\"\"\"\n",
    "\n",
    "    # Berechne Vorhersagen\n",
    "    predictions = w * X + b\n",
    "\n",
    "    # Plotten\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, y, color='blue', s=100, alpha=0.6, label='Daten')\n",
    "    plt.plot(X, predictions, color='red', linewidth=3, label='Regression')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Lineare Regression: y = {w:.4f}x + {b:.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_regression(X_norm, y_norm, w_final, b_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05311a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
