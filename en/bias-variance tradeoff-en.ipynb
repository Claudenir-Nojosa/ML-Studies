{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f114e0",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-off, Overfitting and Underfitting\n",
    "\n",
    "![Capa](../Assets/bias_variance_tradeoff/capa3.png)\n",
    "\n",
    "## 1. The Fundamental Problem\n",
    "\n",
    "When we train a Machine Learning model, we want it to work well on new data (that it has never seen). But there are two types of errors that can occur:\n",
    "\n",
    "1. Error on the Training Set (training data)\n",
    "2. Error on the Test Set (new data)\n",
    "\n",
    "The ideal model has low error on both. But in practice, there is a trade-off between two concepts: Bias and Variance.\n",
    "\n",
    "## 2. What is Bias?\n",
    "\n",
    "Bias is the error caused by wrong assumptions or an overly simple model.\n",
    "\n",
    "### Practical Analogy\n",
    "\n",
    "Imagine you are trying to hit a target with arrows:\n",
    "- High Bias = Your arrows consistently hit far from the center (you are aiming wrong)\n",
    "- Low Bias = Your arrows, on average, hit close to the center\n",
    "\n",
    "### In Machine Learning\n",
    "\n",
    "A model with high bias:\n",
    "- Is too simple to capture the patterns in the data\n",
    "- Makes strong assumptions about the relationship between X and y\n",
    "- Results in Underfitting\n",
    "\n",
    "Example: Using a straight line to model data with a curve\n",
    "\n",
    "![High Bias](../Assets/bias_variance_tradeoff/1.jpg)\n",
    "\n",
    "```\n",
    "Model too simple!\n",
    "Does not capture the real pattern.\n",
    "```\n",
    "\n",
    "## 3. What is Variance?\n",
    "\n",
    "Variance is the error caused by excessive sensitivity to training data.\n",
    "\n",
    "### Practical Analogy\n",
    "\n",
    "Continuing with the arrows:\n",
    "- High Variance = Your arrows are scattered all over the place (inconsistent)\n",
    "- Low Variance = Your arrows are grouped close to each other (consistent)\n",
    "\n",
    "### In Machine Learning\n",
    "\n",
    "A model with high variance:\n",
    "- Is too complex and \"memorizes\" the training data\n",
    "- Adapts too much to the noise in the data\n",
    "- Results in Overfitting\n",
    "\n",
    "Example: Using a degree 10 polynomial to model simple data\n",
    "\n",
    "![High Variance](../Assets/bias_variance_tradeoff/2.png)\n",
    "\n",
    "``` \n",
    "Model too complex!\n",
    "Passes through all points but \n",
    "does not generalize to new data.\n",
    "```\n",
    "\n",
    "## 4. Underfitting - High Bias\n",
    "\n",
    "### What is it?\n",
    "\n",
    "Underfitting happens when the model is too simple to capture the patterns in the data.\n",
    "\n",
    "### Characteristics\n",
    "\n",
    "- High error on training set\n",
    "- High error on test set\n",
    "- Model did not learn the basic pattern of the data\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "Dataset with quadratic relationship: $y = x^2 + \\text{noise}$\n",
    "\n",
    "| x  | y (actual) |\n",
    "|----|----------|\n",
    "| 1  | 1.2      |\n",
    "| 2  | 4.1      |\n",
    "| 3  | 9.3      |\n",
    "| 4  | 16.2     |\n",
    "| 5  | 25.1     |\n",
    "\n",
    "Model 1: Straight line $h(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "Result:\n",
    "- Training Error: 45.2\n",
    "- Test Error: 47.8\n",
    "\n",
    "Why? A straight line cannot capture the curvature of the data!\n",
    "\n",
    "### How to Identify Underfitting?\n",
    "\n",
    "1. High training error (approximately 40-50% error)\n",
    "2. Test error similar to training (small difference)\n",
    "3. Learning curve: both errors remain high even with more data\n",
    "\n",
    "## 5. Overfitting - High Variance\n",
    "\n",
    "### What is it?\n",
    "\n",
    "Overfitting happens when the model is too complex and \"memorizes\" the training data, including the noise.\n",
    "\n",
    "### Characteristics\n",
    "\n",
    "- Low error on training set\n",
    "- High error on test set\n",
    "- Model memorized instead of learning\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "Dataset with quadratic relationship: $y = x^2 + \\text{noise}$\n",
    "\n",
    "| x  | y (train) | y (test) |\n",
    "|----|------------|----------|\n",
    "| 1  | 1.2        | 0.9      |\n",
    "| 2  | 4.1        | 3.8      |\n",
    "| 3  | 9.3        | 9.5      |\n",
    "| 4  | 16.2       | 15.7     |\n",
    "| 5  | 25.1       | 25.4     |\n",
    "\n",
    "Model 2: Degree 10 polynomial $h(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + ... + \\theta_{10} x^{10}$\n",
    "\n",
    "Result:\n",
    "- Training Error: 0.01 (practically zero!)\n",
    "- Test Error: 152.7 (exploded!)\n",
    "\n",
    "Why? The model fitted perfectly to the training data (including noise), but does not generalize to new data.\n",
    "\n",
    "### How to Identify Overfitting?\n",
    "\n",
    "1. Very low training error (approximately 1-5% error)\n",
    "2. Very high test error (10x greater than training)\n",
    "3. Large gap between training and test error\n",
    "4. Learning curve: training continues dropping, test starts rising\n",
    "\n",
    "## 6. The Ideal Model - Just Right (Goldilocks)\n",
    "\n",
    "### Characteristics\n",
    "\n",
    "- Low error on training set\n",
    "- Low error on test set\n",
    "- Small gap between the two\n",
    "\n",
    "### Numerical Example\n",
    "\n",
    "Model 3: Degree 2 polynomial $h(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2$\n",
    "\n",
    "Result:\n",
    "- Training Error: 2.1\n",
    "- Test Error: 2.8\n",
    "- Gap: only 0.7\n",
    "\n",
    "Perfect! Captures the real pattern (quadratic) without memorizing the noise.\n",
    "\n",
    "## 7. Bias-Variance Trade-off\n",
    "\n",
    "### The Total Error Equation\n",
    "\n",
    "$$\\text{Total Error} = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}$$\n",
    "\n",
    "Where:\n",
    "- Bias²: error from a too simple model\n",
    "- Variance: error from a too sensitive model\n",
    "- Irreducible noise: inherent error in the data (unavoidable)\n",
    "\n",
    "### The Trade-off\n",
    "\n",
    "![Bias-Variance Trade-off](../Assets/bias_variance_tradeoff/3.png)\n",
    "\n",
    "### Inverse Relationship\n",
    "\n",
    "Increasing complexity:\n",
    "- Bias decreases (captures complex patterns)\n",
    "- Variance increases (sensitive to noise)\n",
    "  \n",
    "Decreasing complexity:\n",
    "- Bias increases (does not capture patterns)\n",
    "- Variance decreases (more stable)\n",
    "\n",
    "## 8. How to Diagnose the Problem?\n",
    "\n",
    "### Comparing Errors\n",
    "\n",
    "| Situation | Training Error | Test Error | Gap | Diagnosis |\n",
    "|----------|---------------|------------|-----|-------------|\n",
    "| A        | 45%           | 47%        | 2%  | Underfitting (high bias) |\n",
    "| B        | 2%            | 3%         | 1%  | Just Right |\n",
    "| C        | 1%            | 25%        | 24% | Overfitting (high variance) |\n",
    "\n",
    "## 9. How to Fix Underfitting (High Bias)?\n",
    "\n",
    "### Solutions\n",
    "\n",
    "### 1. Increase Model Complexity\n",
    "\n",
    "Before:\n",
    "```python\n",
    "# Model too simple\n",
    "h(x) = θ₀ + θ₁x  # Straight line\n",
    "```\n",
    "\n",
    "After:\n",
    "```python\n",
    "# More complex model\n",
    "h(x) = θ₀ + θ₁x + θ₂x²  # Parabola\n",
    "```\n",
    "\n",
    "### 2. Add More Features\n",
    "\n",
    "Before:\n",
    "```python\n",
    "# Only 1 feature\n",
    "X = [size]\n",
    "```\n",
    "\n",
    "After:\n",
    "```python\n",
    "# Multiple features\n",
    "X = [size, bedrooms, age, location]\n",
    "```\n",
    "\n",
    "### 3. Feature Engineering\n",
    "\n",
    "Create derived features:\n",
    "```python\n",
    "# Original features\n",
    "x₁ = size\n",
    "\n",
    "# Derived features\n",
    "x₂ = size²\n",
    "x₃ = size³\n",
    "x₄ = sqrt(size)\n",
    "```\n",
    "\n",
    "### 4. Remove Regularization\n",
    "\n",
    "If you are using regularization (λ), decrease or remove it:\n",
    "```python\n",
    "# Before: λ too high\n",
    "λ = 10  # Forces simple model\n",
    "\n",
    "# After: λ smaller or zero\n",
    "λ = 0  # Allows more flexible model\n",
    "```\n",
    "\n",
    "### 5. Train for Longer\n",
    "\n",
    "For neural networks, increase epochs:\n",
    "```python\n",
    "# Before\n",
    "epochs = 10  # Stopped early\n",
    "\n",
    "# After\n",
    "epochs = 100  # Trained longer\n",
    "```\n",
    "\n",
    "### Caution!\n",
    "\n",
    "When fixing underfitting, you may cause overfitting. Always monitor the test error!\n",
    "\n",
    "## 10. How to Fix Overfitting (High Variance)?\n",
    "\n",
    "### Solutions\n",
    "\n",
    "### 1. Collect More Data\n",
    "\n",
    "The best solution! More data helps the model generalize better.\n",
    "\n",
    "Before:\n",
    "```python\n",
    "m = 100  # Few examples\n",
    "```\n",
    "\n",
    "After:\n",
    "```python\n",
    "m = 10000  # Many examples\n",
    "```\n",
    "\n",
    "Why does it work? With more data, the model cannot \"memorize\" everything, forcing it to learn real patterns.\n",
    "\n",
    "### 2. Reduce Model Complexity\n",
    "\n",
    "Before:\n",
    "```python\n",
    "# Degree 10 polynomial\n",
    "h(x) = θ₀ + θ₁x + θ₂x² + ... + θ₁₀x¹⁰\n",
    "```\n",
    "\n",
    "After:\n",
    "```python\n",
    "# Degree 2 polynomial\n",
    "h(x) = θ₀ + θ₁x + θ₂x²\n",
    "```\n",
    "\n",
    "### 3. Regularization (L1 or L2)\n",
    "\n",
    "Add penalty to large weights:\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2$$\n",
    "\n",
    "```python\n",
    "# λ controls how much we penalize\n",
    "λ = 0.1   # Moderate regularization\n",
    "λ = 1.0   # Strong regularization\n",
    "λ = 10.0  # Very strong regularization\n",
    "```\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|$$\n",
    "\n",
    "Effect: Forces the weights $\\theta$ to be small, making the model simpler.\n",
    "\n",
    "### 4. Feature Selection (Remove Features)\n",
    "\n",
    "Before:\n",
    "```python\n",
    "# Many features (20)\n",
    "X = [size, bedrooms, age, location, ..., feature_20]\n",
    "```\n",
    "\n",
    "After:\n",
    "```python\n",
    "# Only important features (5)\n",
    "X = [size, bedrooms, location, age, bathrooms]\n",
    "```\n",
    "\n",
    "### 5. Cross-Validation\n",
    "\n",
    "Divide data into k-folds for validation:\n",
    "\n",
    "![k-folds](../Assets/bias_variance_tradeoff/4.png)\n",
    "\n",
    "## 11. Summary - Decision Table\n",
    "\n",
    "| Problem | Symptoms | Solutions |\n",
    "|----------|----------|----------|\n",
    "| Underfitting | High training error<br>High test error<br>Small gap | 1. Increase model complexity<br>2. Add more features<br>3. Feature engineering<br>4. Decrease regularization (λ)<br>5. Train longer |\n",
    "| Overfitting | Low training error<br>High test error<br>Large gap | 1. Collect more data<br>2. Reduce model complexity<br>3. Add regularization (L1/L2)<br>4. Remove features<br>5. Cross-validation |\n",
    "| Just Right | Low training error<br>Low test error<br>Small gap | Keep it up! |\n",
    "\n",
    "## 12. Complete Practical Example\n",
    "\n",
    "### Dataset: Predicting house prices\n",
    "\n",
    "```python\n",
    "# Data\n",
    "X_train: 80 houses\n",
    "y_train: prices\n",
    "\n",
    "X_test: 20 houses\n",
    "y_test: prices\n",
    "```\n",
    "\n",
    "### Attempt 1: Simple Line\n",
    "\n",
    "```python\n",
    "model = LinearRegression()  # h(x) = θ₀ + θ₁x\n",
    "```\n",
    "\n",
    "Result:\n",
    "- Training Error: 42%\n",
    "- Test Error: 45%\n",
    "- Diagnosis: UNDERFITTING\n",
    "\n",
    "Action: Increase complexity\n",
    "\n",
    "### Attempt 2: Degree 2 Polynomial\n",
    "\n",
    "```python\n",
    "model = PolynomialRegression(degree=2)  # h(x) = θ₀ + θ₁x + θ₂x²\n",
    "```\n",
    "\n",
    "Result:\n",
    "- Training Error: 5%\n",
    "- Test Error: 8%\n",
    "- Diagnosis: JUST RIGHT\n",
    "\n",
    "Action: Success! Balanced model.\n",
    "\n",
    "### Attempt 3: Degree 10 Polynomial\n",
    "\n",
    "```python\n",
    "model = PolynomialRegression(degree=10)\n",
    "```\n",
    "\n",
    "Result:\n",
    "- Training Error: 0.5%\n",
    "- Test Error: 45%\n",
    "- Diagnosis: OVERFITTING\n",
    "\n",
    "Action: Apply regularization\n",
    "\n",
    "### Attempt 4: Degree 10 Polynomial + Regularization\n",
    "\n",
    "```python\n",
    "model = Ridge(degree=10, alpha=1.0)  # α = λ (regularization)\n",
    "```\n",
    "\n",
    "Result:\n",
    "- Training Error: 4%\n",
    "- Test Error: 6%\n",
    "- Diagnosis: JUST RIGHT\n",
    "\n",
    "Action: Success! Regularization solved it.\n",
    "\n",
    "## 13. Final Tips\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. Always separate train/test (80/20 or 70/30)\n",
    "2. Use cross-validation to choose hyperparameters\n",
    "3. Start simple, add complexity gradually\n",
    "4. Monitor both errors (training and test)\n",
    "5. Plot learning curves to visualize\n",
    "\n",
    "### Common Mistakes\n",
    "\n",
    "1. Not separating test set (training and testing on same data)\n",
    "2. Using test set to tune model (data leakage)\n",
    "3. Excessive complexity from the start\n",
    "4. Ignoring training error (focusing only on test)\n",
    "5. Not using regularization when appropriate\n",
    "\n",
    "---\n",
    "\n",
    "Remember:\n",
    "_\"The best model is not the one that best fits the training data, but the one that best generalizes to new data.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21c1d2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
