{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910e74e8",
   "metadata": {},
   "source": [
    "# Linear Regression and Gradient Descent - Complete Guide\n",
    "\n",
    "![Cover](Assets/Supervised_Learning/Capa.png)\n",
    "\n",
    "\n",
    "## 1. Supervised Learning\n",
    "\n",
    "The **Supervised Learning** process consists of taking the **Training Set** and feeding this data to our **Learning Algorithm**. The algorithm's job is to present us with a function that makes predictions.\n",
    "\n",
    "By convention, this function is called a **hypothesis**. The hypothesis's job is to take information (features) it hasn't seen yet and estimate the output correctly.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Notation and Fundamental Concepts\n",
    "\n",
    "### Dataset Notation\n",
    "\n",
    "- $m$ = number of training examples (number of rows in the table)\n",
    "- $n$ = number of features (input variables)\n",
    "- $x$ = *inputs* \n",
    "- $y$ = *output* / target value (what we want to predict)\n",
    "- $(x, y)$ = a training example\n",
    "- $(x^{(i)}, y^{(i)})$ = the i-th training example\n",
    "\n",
    "### Parameters\n",
    "\n",
    "$\\theta$ (theta) are called **parameters**. The learning algorithm's job is to choose the parameters $\\theta$ that allow for good predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Example Dataset\n",
    "\n",
    "![Example Dataset](Assets/Supervised_Learning/1.png)\n",
    "\n",
    "Let's use a super simple dataset with only **3 houses** to understand the calculations:\n",
    "\n",
    "| Size (m¬≤) | Price (thousand R$) |\n",
    "|-----------|---------------------|\n",
    "| 50        | 150                 |\n",
    "| 80        | 200                 |\n",
    "| 110       | 250                 |\n",
    "\n",
    "So we have:\n",
    "- $m = 3$ (3 training examples)\n",
    "- $n = 1$ (1 feature: size)\n",
    "- $x^{(1)} = 50, \\quad y^{(1)} = 150$\n",
    "- $x^{(2)} = 80, \\quad y^{(2)} = 200$\n",
    "- $x^{(3)} = 110, \\quad y^{(3)} = 250$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hypothesis (Our Line)\n",
    "\n",
    "### Basic Representation\n",
    "\n",
    "In **Linear Regression** with one feature, the hypothesis is represented as:\n",
    "\n",
    "$$h(x) = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "![Hypothesis](Assets/Supervised_Learning/2.png)\n",
    "\n",
    "In the example above, $\\theta_0$ = 1.5 and $\\theta_1$ = 0\n",
    "\n",
    "We observe that it closely resembles a first-degree equation, where $f(x) = b + ax$\n",
    "\n",
    "Where:\n",
    "- $\\theta_0$ = intercept (where the line crosses the y-axis)\n",
    "- $\\theta_1$ = slope (angular coefficient)\n",
    "- $x$ = input (size), i.e., what we want to use to make predictions\n",
    "\n",
    "**Numerical example:** If $\\theta_0 = 50$ and $\\theta_1 = 2$, then:\n",
    "\n",
    "$$h(x) = 50 + 2x$$\n",
    "\n",
    "For an 80m¬≤ house:\n",
    "$$h(80) = 50 + 2(80) = 50 + 160 = 210 \\text{ thousand R\\$}$$\n",
    "\n",
    "### Multiple Features\n",
    "\n",
    "When we have more than one feature (variable), such as the number of bedrooms for example:\n",
    "\n",
    "$$h(x) = \\theta_0 + \\theta_1x_1 + \\theta_2x_2$$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $x_1$ = size\n",
    "- $x_2$ = #bedrooms\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Cost Function - Measuring Error\n",
    "\n",
    "The **Cost Function** $J(\\theta)$ measures how far our predictions are from the actual values. It's the difference between the actual data value ($y$) minus the value predicted by the line ($h(x)$), squared, summed across all data points:\n",
    "\n",
    "$$J(\\theta_0, \\theta_1) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "The intention is to make this difference as small as possible.\n",
    "\n",
    "Formula to find $\\theta_0$ and $\\theta_1$:\n",
    "\n",
    "$$\n",
    "\\theta_1\n",
    "=\n",
    "\\frac\n",
    "{\n",
    "\\sum_{i=1}^{m}\n",
    "\\left(x^{(i)} - \\bar{x}\\right)\n",
    "\\left(y^{(i)} - \\bar{y}\\right)\n",
    "}\n",
    "{\n",
    "\\sum_{i=1}^{m}\n",
    "\\left(x^{(i)} - \\bar{x}\\right)^2\n",
    "}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\theta_0 = \\bar{y} - \\theta_1 \\bar{x}\n",
    "$$\n",
    "\n",
    "\n",
    "### Step-by-Step Calculation Example\n",
    "\n",
    "Let's calculate $J(\\theta_0, \\theta_1)$ for $\\theta_0 = 50$ and $\\theta_1 = 2$ using our dataset:\n",
    "\n",
    "**Step 1:** Calculate predictions $h(x^{(i)})$\n",
    "- $h(x^{(1)}) = 50 + 2(50) = 150$\n",
    "- $h(x^{(2)}) = 50 + 2(80) = 210$\n",
    "- $h(x^{(3)}) = 50 + 2(110) = 270$\n",
    "\n",
    "**Step 2:** Calculate errors $(h(x^{(i)}) - y^{(i)})$\n",
    "- Error 1: $150 - 150 = 0$\n",
    "- Error 2: $210 - 200 = 10$\n",
    "- Error 3: $270 - 250 = 20$\n",
    "\n",
    "**Step 3:** Square the errors\n",
    "- Error¬≤ 1: $0^2 = 0$\n",
    "- Error¬≤ 2: $10^2 = 100$\n",
    "- Error¬≤ 3: $20^2 = 400$\n",
    "\n",
    "**Step 4:** Sum and divide by $2m$\n",
    "\n",
    "$$J(50, 2) = \\frac{1}{2(3)}(0 + 100 + 400) = \\frac{500}{6} = 83.33$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Gradient Descent - Finding the Best Parameters\n",
    "\n",
    "![Gradient Descent](Assets/Supervised_Learning/3.png)\n",
    "\n",
    "\n",
    "To minimize $J(\\theta)$, we use **Gradient Descent**. Basically, we look in 360¬∞ and search for values of $\\theta_0$ and $\\theta_1$ to find the smallest $J(\\theta)$ as quickly as possible.\n",
    "\n",
    "### Gradient Descent Algorithm\n",
    "\n",
    "Repeat until convergence:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n",
    "\n",
    "Where:\n",
    "- $:=$ means *assignment*, i.e., the value is updated by a new one\n",
    "- $\\alpha$ is the **learning rate**\n",
    "- $\\frac{\\partial}{\\partial \\theta_j} J(\\theta)$ is the partial derivative of the cost function\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "- In practice, the learning rate is generally set as $\\alpha = 0.01$\n",
    "- The derivative of a function defines the direction of the *steepest descent*, i.e., going *downhill* as fast as possible\n",
    "\n",
    "### Partial Derivatives for Linear Regression\n",
    "\n",
    "For Linear Regression, the derivatives are:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_0} J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_1} J(\\theta_0, \\theta_1) = \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}$$\n",
    "\n",
    "### Detailed Numerical Example\n",
    "\n",
    "Let's execute Gradient Descent manually with our dataset!\n",
    "\n",
    "**Initial values:**\n",
    "- $\\theta_0 = 0$\n",
    "- $\\theta_1 = 0$\n",
    "- $\\alpha = 0.01$ (learning rate)\n",
    "\n",
    "#### Iteration 1\n",
    "\n",
    "**Step 1:** Calculate predictions with $\\theta_0 = 0, \\theta_1 = 0$\n",
    "- $h(50) = 0 + 0(50) = 0$\n",
    "- $h(80) = 0 + 0(80) = 0$\n",
    "- $h(110) = 0 + 0(110) = 0$\n",
    "\n",
    "**Step 2:** Calculate errors\n",
    "- Error 1: $0 - 150 = -150$\n",
    "- Error 2: $0 - 200 = -200$\n",
    "- Error 3: $0 - 250 = -250$\n",
    "\n",
    "**Step 3:** Calculate derivative for $\\theta_0$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_0} = \\frac{1}{3}(-150 - 200 - 250) = \\frac{-600}{3} = -200$$\n",
    "\n",
    "**Step 4:** Calculate derivative for $\\theta_1$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial \\theta_1} = \\frac{1}{3}[(-150)(50) + (-200)(80) + (-250)(110)]$$\n",
    "$$= \\frac{1}{3}[-7500 - 16000 - 27500] = \\frac{-51000}{3} = -17000$$\n",
    "\n",
    "**Step 5:** Update parameters\n",
    "\n",
    "$$\\theta_0 := 0 - 0.01(-200) = 0 + 2 = 2$$\n",
    "$$\\theta_1 := 0 - 0.01(-17000) = 0 + 170 = 170$$\n",
    "\n",
    "Now we have: $\\theta_0 = 2$ and $\\theta_1 = 170$\n",
    "\n",
    "#### Iteration 2\n",
    "\n",
    "**Step 1:** Calculate predictions with $\\theta_0 = 2, \\theta_1 = 170$\n",
    "- $h(50) = 2 + 170(50) = 8502$\n",
    "- $h(80) = 2 + 170(80) = 13602$\n",
    "- $h(110) = 2 + 170(110) = 18702$\n",
    "\n",
    "**Step 2:** Calculate errors\n",
    "- Error 1: $8502 - 150 = 8352$\n",
    "- Error 2: $13602 - 200 = 13402$\n",
    "- Error 3: $18702 - 250 = 18452$\n",
    "\n",
    "**Observe:** The values are very far off! This happens because $\\alpha$ multiplied by large derivatives makes enormous \"jumps\". With more iterations, the values will converge.\n",
    "\n",
    "**With many iterations**, the values converge to the optimal values!\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Types of Gradient Descent\n",
    "\n",
    "### Batch Gradient Descent\n",
    "\n",
    "![Batch Gradient Descent](Assets/Supervised_Learning/4.png)\n",
    "\n",
    "Uses **all the data** in each iteration. This is what we did above:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}$$\n",
    "\n",
    "Each gradient descent step requires going through the **entire dataset**. This is not good when we have a very large dataset, as it becomes very time-consuming.\n",
    "\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "![Stochastic Gradient Descent](Assets/Supervised_Learning/5.png)\n",
    "\n",
    "To overcome the Batch GD problem, there's **Stochastic Gradient Descent**, which uses **only 1 example** at a time, randomly chosen:\n",
    "\n",
    "$$\\theta_j := \\theta_j - \\alpha (h(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)}$$\n",
    "\n",
    "SGD takes a random house, predicts the price, and adjusts the parameters with another random house, testing iteratively until it really finds or gets very close to the *global optimum*.\n",
    "\n",
    "**Advantage:** Much faster for large datasets!\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Final Result\n",
    "\n",
    "After many iterations (about 100), Gradient Descent finds the optimal values. For our dataset, the ideal values are approximately:\n",
    "\n",
    "$$\\theta_0 \\approx 83.33$$\n",
    "$$\\theta_1 \\approx 1.67$$\n",
    "\n",
    "So our final line is:\n",
    "\n",
    "$$h(x) = 83.33 + 1.67x$$\n",
    "\n",
    "**Prediction example:** To predict the price of a 90m¬≤ house:\n",
    "\n",
    "$$h(90) = 83.33 + 1.67(90) = 83.33 + 150.3 = 233.63 \\text{ thousand R\\$}$$\n",
    "\n",
    "---\n",
    "\n",
    "Below will be an example code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca0af5",
   "metadata": {},
   "source": [
    "# Code Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec25b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# 1. DATASET - 3 HOUSES\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"1. EXAMPLE DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Our 3 data points\n",
    "X = np.array([50, 80, 110])   # Size (m¬≤)\n",
    "y = np.array([150, 200, 250])  # Price (thousand R$)\n",
    "m = len(X)  # number of examples (m)\n",
    "n = 1       # number of features (n)\n",
    "\n",
    "print(f\"Number of examples (m): {m}\")\n",
    "print(f\"Number of features (n): {n}\")\n",
    "print(f\"\\nData:\")\n",
    "for i in range(m):\n",
    "    print(f\"  x^({i+1}) = {X[i]:3d} m¬≤  ‚Üí  y^({i+1}) = {y[i]:3d} thousand R$\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e172a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "2. HYPOTHESIS h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅx\n",
      "============================================================\n",
      "Example: h(x) = 50 + 2x\n",
      "For x = 80m¬≤:\n",
      "h(80) = 50 + 2(80) = 210 thousand R$\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 2. HYPOTHESIS FUNCTION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"2. HYPOTHESIS h(x) = Œ∏‚ÇÄ + Œ∏‚ÇÅx\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def hypothesis(X, theta0, theta1):\n",
    "    \"\"\"Calculate h(x) = theta0 + theta1 * x\"\"\"\n",
    "    return theta0 + theta1 * X\n",
    "\n",
    "\n",
    "# Example with Œ∏‚ÇÄ = 50, Œ∏‚ÇÅ = 2\n",
    "theta0_example = 50\n",
    "theta1_example = 2\n",
    "x_example = 80\n",
    "\n",
    "h_example = hypothesis(x_example, theta0_example, theta1_example)\n",
    "print(f\"Example: h(x) = {theta0_example} + {theta1_example}x\")\n",
    "print(f\"For x = {x_example}m¬≤:\")\n",
    "print(f\"h({x_example}) = {theta0_example} + {theta1_example}({x_example}) = {h_example} thousand R$\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "716f3b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "3. COST FUNCTION J(Œ∏‚ÇÄ, Œ∏‚ÇÅ)\n",
      "============================================================\n",
      "Calculating J(50, 2):\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Example of manual calculation with Œ∏‚ÇÄ=50, Œ∏‚ÇÅ=2\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating J(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta0_example\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta1_example\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 24\u001b[0m predictions \u001b[38;5;241m=\u001b[39m hypothesis(\u001b[43mX\u001b[49m, theta0_example, theta1_example)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep 1 - Predictions h(x^(i)):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 3. COST FUNCTION (LOSS FUNCTION)\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"3. COST FUNCTION J(Œ∏‚ÇÄ, Œ∏‚ÇÅ)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def cost_function(X, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Calculate J(theta0, theta1) = (1/2m) * sum((h(x) - y)¬≤)\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    predictions = hypothesis(X, theta0, theta1)\n",
    "    errors = predictions - y\n",
    "    squared_errors = errors ** 2\n",
    "    cost = (1 / (2 * m)) * np.sum(squared_errors)\n",
    "    return cost\n",
    "\n",
    "\n",
    "# Example of manual calculation with Œ∏‚ÇÄ=50, Œ∏‚ÇÅ=2\n",
    "print(f\"Calculating J({theta0_example}, {theta1_example}):\\n\")\n",
    "\n",
    "predictions = hypothesis(X, theta0_example, theta1_example)\n",
    "print(\"Step 1 - Predictions h(x^(i)):\")\n",
    "for i in range(m):\n",
    "    print(\n",
    "        f\"  h(x^({i+1})) = {theta0_example} + {theta1_example}({X[i]}) = {predictions[i]:.0f}\")\n",
    "\n",
    "errors = predictions - y\n",
    "print(\"\\nStep 2 - Errors (h(x^(i)) - y^(i)):\")\n",
    "for i in range(m):\n",
    "    print(f\"  Error {i+1}: {predictions[i]:.0f} - {y[i]} = {errors[i]:.0f}\")\n",
    "\n",
    "squared_errors = errors ** 2\n",
    "print(\"\\nStep 3 - Squared errors:\")\n",
    "for i in range(m):\n",
    "    print(f\"  Error¬≤ {i+1}: ({errors[i]:.0f})¬≤ = {squared_errors[i]:.0f}\")\n",
    "\n",
    "cost = cost_function(X, y, theta0_example, theta1_example)\n",
    "print(f\"\\nStep 4 - Sum and divide by 2m:\")\n",
    "print(\n",
    "    f\"  J({theta0_example}, {theta1_example}) = (1/6)({squared_errors[0]:.0f} + {squared_errors[1]:.0f} + {squared_errors[2]:.0f})\")\n",
    "print(f\"  J({theta0_example}, {theta1_example}) = {cost:.2f}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2cce7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "4. GRADIENT DESCENT\n",
      "============================================================\n",
      "‚ö†Ô∏è  IMPORTANT: CHOOSING THE CORRECT LEARNING RATE\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "For this dataset (large values: 50-250), we need\n",
      "a VERY SMALL learning rate to avoid divergence!\n",
      "\n",
      "Tested values:\n",
      "  Œ± = 0.01    ‚Üí EXPLODES! ‚ùå\n",
      "  Œ± = 0.001   ‚Üí EXPLODES! ‚ùå\n",
      "  Œ± = 0.0001  ‚Üí Converges very slowly\n",
      "  Œ± = 0.00001 ‚Üí Converges well! ‚úì\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 102\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Œ± = 0.00001 ‚Üí Converges well! ‚úì\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Run Gradient Descent with appropriate learning rate\u001b[39;00m\n\u001b[0;32m    101\u001b[0m theta0_final, theta1_final, history \u001b[38;5;241m=\u001b[39m gradient_descent(\n\u001b[1;32m--> 102\u001b[0m     \u001b[43mX\u001b[49m, y,\n\u001b[0;32m    103\u001b[0m     theta0_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    104\u001b[0m     theta1_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    105\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.00001\u001b[39m,  \u001b[38;5;66;03m# CORRECT learning rate!\u001b[39;00m\n\u001b[0;32m    106\u001b[0m     iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m,  \u001b[38;5;66;03m# More iterations needed\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    108\u001b[0m )\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINAL RESULT AFTER \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(history)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ITERATIONS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 4. GRADIENT DESCENT - IMPLEMENTATION\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"4. GRADIENT DESCENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def compute_gradients(X, y, theta0, theta1):\n",
    "    \"\"\"\n",
    "    Calculate the partial derivatives:\n",
    "    ‚àÇJ/‚àÇŒ∏‚ÇÄ = (1/m) * sum(h(x) - y)\n",
    "    ‚àÇJ/‚àÇŒ∏‚ÇÅ = (1/m) * sum((h(x) - y) * x)\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    predictions = hypothesis(X, theta0, theta1)\n",
    "    errors = predictions - y\n",
    "\n",
    "    d_theta0 = (1/m) * np.sum(errors)\n",
    "    d_theta1 = (1/m) * np.sum(errors * X)\n",
    "\n",
    "    return d_theta0, d_theta1\n",
    "\n",
    "\n",
    "def gradient_descent(X, y, theta0_init, theta1_init, alpha, iterations, verbose=True):\n",
    "    \"\"\"\n",
    "    Execute Gradient Descent (BATCH)\n",
    "\n",
    "    Œ∏‚±º := Œ∏‚±º - Œ± * (‚àÇJ/‚àÇŒ∏‚±º)\n",
    "    \"\"\"\n",
    "    theta0 = theta0_init\n",
    "    theta1 = theta1_init\n",
    "    m = len(X)\n",
    "    history = []\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Initial values: Œ∏‚ÇÄ = {theta0}, Œ∏‚ÇÅ = {theta1}\")\n",
    "        print(f\"Learning rate (Œ±): {alpha}\")\n",
    "        print(f\"Iterations: {iterations}\\n\")\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Calculate predictions\n",
    "        predictions = hypothesis(X, theta0, theta1)\n",
    "\n",
    "        # Calculate errors\n",
    "        errors = predictions - y\n",
    "\n",
    "        # Calculate partial derivatives (gradients)\n",
    "        d_theta0, d_theta1 = compute_gradients(X, y, theta0, theta1)\n",
    "\n",
    "        # Update parameters using gradient descent rule\n",
    "        theta0 = theta0 - alpha * d_theta0\n",
    "        theta1 = theta1 - alpha * d_theta1\n",
    "\n",
    "        # Calculate cost\n",
    "        cost = cost_function(X, y, theta0, theta1)\n",
    "\n",
    "        # Save to history\n",
    "        history.append((theta0, theta1, cost))\n",
    "\n",
    "        # Show detailed progress for the first 2 iterations\n",
    "        if verbose and i < 2:\n",
    "            print(f\"{'‚îÄ' * 50}\")\n",
    "            print(f\"ITERATION {i+1}\")\n",
    "            print(f\"{'‚îÄ' * 50}\")\n",
    "            print(\n",
    "                f\"Predictions h(x): [{predictions[0]:.2f}, {predictions[1]:.2f}, {predictions[2]:.2f}]\")\n",
    "            print(\n",
    "                f\"Errors (h(x) - y): [{errors[0]:.2f}, {errors[1]:.2f}, {errors[2]:.2f}]\")\n",
    "            print(f\"\\nPartial derivatives:\")\n",
    "            print(\n",
    "                f\"  ‚àÇJ/‚àÇŒ∏‚ÇÄ = (1/{m}) √ó ({errors[0]:.2f} + {errors[1]:.2f} + {errors[2]:.2f})\")\n",
    "            print(f\"         = {d_theta0:.4f}\")\n",
    "            print(\n",
    "                f\"\\n  ‚àÇJ/‚àÇŒ∏‚ÇÅ = (1/{m}) √ó ({errors[0]:.2f}√ó{X[0]} + {errors[1]:.2f}√ó{X[1]} + {errors[2]:.2f}√ó{X[2]})\")\n",
    "            print(f\"         = {d_theta1:.4f}\")\n",
    "            print(f\"\\nParameter update:\")\n",
    "            prev_theta0 = theta0 + alpha * d_theta0\n",
    "            prev_theta1 = theta1 + alpha * d_theta1\n",
    "            print(\n",
    "                f\"  Œ∏‚ÇÄ := {prev_theta0:.4f} - {alpha} √ó {d_theta0:.4f} = {theta0:.4f}\")\n",
    "            print(\n",
    "                f\"  Œ∏‚ÇÅ := {prev_theta1:.4f} - {alpha} √ó {d_theta1:.4f} = {theta1:.4f}\")\n",
    "            print(f\"\\nCost J(Œ∏) = {cost:.4f}\\n\")\n",
    "\n",
    "    return theta0, theta1, history\n",
    "\n",
    "\n",
    "# ‚ö†Ô∏è IMPORTANT: Correct learning rate!\n",
    "print(\"‚ö†Ô∏è  IMPORTANT: CHOOSING THE CORRECT LEARNING RATE\")\n",
    "print(\"‚îÄ\" * 60)\n",
    "print(\"For this dataset (large values: 50-250), we need\")\n",
    "print(\"a VERY SMALL learning rate to avoid divergence!\\n\")\n",
    "print(\"Tested values:\")\n",
    "print(\"  Œ± = 0.01    ‚Üí EXPLODES! ‚ùå\")\n",
    "print(\"  Œ± = 0.001   ‚Üí EXPLODES! ‚ùå\")\n",
    "print(\"  Œ± = 0.0001  ‚Üí Converges very slowly\")\n",
    "print(\"  Œ± = 0.00001 ‚Üí Converges well! ‚úì\\n\")\n",
    "\n",
    "# Run Gradient Descent with appropriate learning rate\n",
    "theta0_final, theta1_final, history = gradient_descent(\n",
    "    X, y,\n",
    "    theta0_init=0,\n",
    "    theta1_init=0,\n",
    "    alpha=0.00001,  # CORRECT learning rate!\n",
    "    iterations=50000,  # More iterations needed\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"FINAL RESULT AFTER {len(history)} ITERATIONS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Œ∏‚ÇÄ (intercept) = {theta0_final:.4f}\")\n",
    "print(f\"Œ∏‚ÇÅ (slope) = {theta1_final:.4f}\")\n",
    "print(f\"Final cost J(Œ∏) = {history[-1][2]:.4f}\")\n",
    "print(f\"\\nFinal line equation:\")\n",
    "print(f\"h(x) = {theta0_final:.2f} + {theta1_final:.2f}x\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082cc5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "5. FEATURE SCALING - THE SOLUTION FOR LEARNING RATE\n",
      "============================================================\n",
      "\n",
      "üí° Why normalize the data?\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "When X and y values are large (50-250), the gradients\n",
      "become enormous, forcing the use of tiny learning rates.\n",
      "\n",
      "SOLUTION: Normalize data to the scale [0, 1] or [-1, 1]\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSOLUTION: Normalize data to the scale [0, 1] or [-1, 1]\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Normalize using Min-Max scaling\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m X_norm \u001b[38;5;241m=\u001b[39m (\u001b[43mX\u001b[49m \u001b[38;5;241m-\u001b[39m X\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (X\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m X\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m     16\u001b[0m y_norm \u001b[38;5;241m=\u001b[39m (y \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mmin()) \u001b[38;5;241m/\u001b[39m (y\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m y\u001b[38;5;241m.\u001b[39mmin())\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal data:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 5. FEATURE SCALING (NORMALIZATION)\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"5. FEATURE SCALING - THE SOLUTION FOR LEARNING RATE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüí° Why normalize the data?\")\n",
    "print(\"‚îÄ\" * 60)\n",
    "print(\"When X and y values are large (50-250), the gradients\")\n",
    "print(\"become enormous, forcing the use of tiny learning rates.\")\n",
    "print(\"\\nSOLUTION: Normalize data to the scale [0, 1] or [-1, 1]\\n\")\n",
    "\n",
    "# Normalize using Min-Max scaling\n",
    "X_norm = (X - X.min()) / (X.max() - X.min())\n",
    "y_norm = (y - y.min()) / (y.max() - y.min())\n",
    "\n",
    "print(f\"Original data:\")\n",
    "print(f\"  X = {X}\")\n",
    "print(f\"  y = {y}\")\n",
    "print(f\"\\nNormalized data:\")\n",
    "print(f\"  X_norm = {X_norm}\")\n",
    "print(f\"  y_norm = {y_norm}\\n\")\n",
    "\n",
    "# Run GD with normalized data and larger Œ±\n",
    "print(\"Running GD with NORMALIZED data and Œ± = 0.1 (100x larger!):\")\n",
    "theta0_norm, theta1_norm, history_norm = gradient_descent(\n",
    "    X_norm, y_norm,\n",
    "    theta0_init=0,\n",
    "    theta1_init=0,\n",
    "    alpha=0.1,  # Now we can use much larger Œ±!\n",
    "    iterations=1000,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(f\"\\nResult (normalized data):\")\n",
    "print(f\"  Œ∏‚ÇÄ = {theta0_norm:.4f}\")\n",
    "print(f\"  Œ∏‚ÇÅ = {theta1_norm:.4f}\")\n",
    "print(f\"  Final cost = {history_norm[-1][2]:.6f}\")\n",
    "print(f\"  Converged in only 1000 iterations! ‚úì\\n\")\n",
    "\n",
    "# Denormalize parameters to get original equation\n",
    "# h(x) = theta0 + theta1 * x\n",
    "# Denormalization: y = y_min + (y_max - y_min) * y_norm\n",
    "# x_norm = (x - x_min) / (x_max - x_min)\n",
    "theta1_original = theta1_norm * (y.max() - y.min()) / (X.max() - X.min())\n",
    "theta0_original = y.min() + theta0_norm * (y.max() - y.min()) - \\\n",
    "    theta1_original * X.min()\n",
    "\n",
    "print(\"Converting back to original scale:\")\n",
    "print(f\"  Œ∏‚ÇÄ = {theta0_original:.4f}\")\n",
    "print(f\"  Œ∏‚ÇÅ = {theta1_original:.4f}\")\n",
    "print(f\"  (Should be close to Œ∏‚ÇÄ‚âà83.33, Œ∏‚ÇÅ‚âà1.67)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5af9501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "6. MAKING PREDICTIONS WITH TRAINED MODEL\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'theta0_final' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m test_sizes \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m60\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m120\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m size \u001b[38;5;129;01min\u001b[39;00m test_sizes:\n\u001b[1;32m---> 10\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m hypothesis(size, \u001b[43mtheta0_final\u001b[49m, theta1_final)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHouse of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm¬≤ ‚Üí Estimated price: R$ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m thousand\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'theta0_final' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 6. MAKING PREDICTIONS WITH FINAL MODEL\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"6. MAKING PREDICTIONS WITH TRAINED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "test_sizes = [60, 90, 120]\n",
    "for size in test_sizes:\n",
    "    prediction = hypothesis(size, theta0_final, theta1_final)\n",
    "    print(\n",
    "        f\"House of {size:3d}m¬≤ ‚Üí Estimated price: R$ {prediction:.2f} thousand\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5575306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "7. GENERATING PLOTS\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m7. GENERATING PLOTS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m fig \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 7.1 - Original data + Fitted line\u001b[39;00m\n\u001b[0;32m     11\u001b[0m ax1 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 7. VISUALIZATIONS\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"7. GENERATING PLOTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# 7.1 - Original data + Fitted line\n",
    "ax1 = plt.subplot(2, 3, 1)\n",
    "plt.scatter(X, y, color='red', s=150, marker='o',\n",
    "            label='Real data', zorder=3)\n",
    "x_line = np.linspace(40, 120, 100)\n",
    "y_line = hypothesis(x_line, theta0_final, theta1_final)\n",
    "plt.plot(x_line, y_line, color='blue', linewidth=2.5,\n",
    "         label=f'h(x) = {theta0_final:.2f} + {theta1_final:.2f}x')\n",
    "plt.xlabel('Size (m¬≤)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Price (thousand R$)', fontsize=11, fontweight='bold')\n",
    "plt.title('Non-Normalized Data + Fitted Line',\n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7.2 - Convergence of Cost J(Œ∏) - Non-normalized data\n",
    "ax2 = plt.subplot(2, 3, 2)\n",
    "costs = [h[2] for h in history]\n",
    "plt.plot(costs, color='green', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Cost J(Œ∏)', fontsize=11, fontweight='bold')\n",
    "plt.title(f'Convergence (Œ±={0.00001})', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7.3 - Parameter trajectory - Non-normalized data\n",
    "ax3 = plt.subplot(2, 3, 3)\n",
    "theta0_history = [h[0] for h in history]\n",
    "theta1_history = [h[1] for h in history]\n",
    "plt.plot(theta0_history, theta1_history, 'o-', markersize=1,\n",
    "         linewidth=1, alpha=0.6, color='purple')\n",
    "plt.plot(theta0_history[0], theta1_history[0], 'go',\n",
    "         markersize=10, label='Start', zorder=5)\n",
    "plt.plot(theta0_history[-1], theta1_history[-1],\n",
    "         'ro', markersize=10, label='End', zorder=5)\n",
    "plt.xlabel('Œ∏‚ÇÄ', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Œ∏‚ÇÅ', fontsize=11, fontweight='bold')\n",
    "plt.title('Parameter Trajectory', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7.4 - NORMALIZED data + Fitted line\n",
    "ax4 = plt.subplot(2, 3, 4)\n",
    "plt.scatter(X_norm, y_norm, color='red', s=150, marker='o',\n",
    "            label='Normalized data', zorder=3)\n",
    "x_line_norm = np.linspace(0, 1, 100)\n",
    "y_line_norm = hypothesis(x_line_norm, theta0_norm, theta1_norm)\n",
    "plt.plot(x_line_norm, y_line_norm, color='blue', linewidth=2.5)\n",
    "plt.xlabel('Size (normalized)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Price (normalized)', fontsize=11, fontweight='bold')\n",
    "plt.title('NORMALIZED Data + Fitted Line', fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7.5 - Convergence of Cost J(Œ∏) - Normalized data\n",
    "ax5 = plt.subplot(2, 3, 5)\n",
    "costs_norm = [h[2] for h in history_norm]\n",
    "plt.plot(costs_norm, color='green', linewidth=2)\n",
    "plt.xlabel('Iteration', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Cost J(Œ∏)', fontsize=11, fontweight='bold')\n",
    "plt.title(\n",
    "    f'NORMALIZED Convergence (Œ±={0.1})', fontsize=12, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7.6 - Convergence comparison\n",
    "ax6 = plt.subplot(2, 3, 6)\n",
    "# Normalize both costs for comparison\n",
    "costs_sample = costs[::len(costs)//1000] if len(costs) > 1000 else costs\n",
    "costs_norm_sample = costs_norm\n",
    "plt.plot(range(len(costs_sample)), costs_sample,\n",
    "         label='Without normalization', linewidth=2, alpha=0.7)\n",
    "plt.plot(range(len(costs_norm_sample)), costs_norm_sample,\n",
    "         label='With normalization', linewidth=2, alpha=0.7)\n",
    "plt.xlabel('Iteration (adjusted scale)', fontsize=11, fontweight='bold')\n",
    "plt.ylabel('Cost J(Œ∏)', fontsize=11, fontweight='bold')\n",
    "plt.title('Comparison: With vs Without Normalization',\n",
    "          fontsize=12, fontweight='bold')\n",
    "plt.legend(fontsize=9)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Plots generated!\")\n",
    "print(\"  ‚Ä¢ Top row: NON-normalized data\")\n",
    "print(\"  ‚Ä¢ Bottom row: NORMALIZED data\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a02bd95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "8. LEARNING RATE IMPACT (Normalized Data)\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m      8\u001b[0m alphas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m]\n\u001b[1;32m----> 9\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, alpha \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(alphas):\n\u001b[0;32m     12\u001b[0m     _, _, hist \u001b[38;5;241m=\u001b[39m gradient_descent(\n\u001b[0;32m     13\u001b[0m         X_norm, y_norm, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, alpha, \u001b[38;5;241m1000\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 8. COMPARISON: DIFFERENT LEARNING RATES\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"8. LEARNING RATE IMPACT (Normalized Data)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "alphas = [0.01, 0.1, 0.5]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for idx, alpha in enumerate(alphas):\n",
    "    _, _, hist = gradient_descent(\n",
    "        X_norm, y_norm, 0, 0, alpha, 1000, verbose=False)\n",
    "    costs = [h[2] for h in hist]\n",
    "\n",
    "    axes[idx].plot(costs, linewidth=2.5)\n",
    "    axes[idx].set_xlabel('Iteration', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Cost J(Œ∏)', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'Œ± = {alpha}', fontsize=13, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "    final_cost = costs[-1] if not np.isinf(costs[-1]) else \"inf\"\n",
    "    if isinstance(final_cost, float):\n",
    "        axes[idx].text(0.98, 0.98, f'Final: {final_cost:.6f}',\n",
    "                       transform=axes[idx].transAxes,\n",
    "                       fontsize=9, verticalalignment='top', horizontalalignment='right',\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "    print(f\"Œ± = {alpha:4.2f}  ‚Üí  Final cost = {final_cost if isinstance(final_cost, str) else f'{final_cost:.6f}'}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c09bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "9. STOCHASTIC GRADIENT DESCENT\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m theta0, theta1, history\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Run SGD with normalized data\u001b[39;00m\n\u001b[0;32m     40\u001b[0m theta0_sgd, theta1_sgd, history_sgd \u001b[38;5;241m=\u001b[39m stochastic_gradient_descent(\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mX_norm\u001b[49m, y_norm, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m     42\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComparison Batch GD vs Stochastic GD (normalized data):\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBatch GD:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# 9. STOCHASTIC GRADIENT DESCENT (SGD)\n",
    "# ============================================\n",
    "print(\"=\" * 60)\n",
    "print(\"9. STOCHASTIC GRADIENT DESCENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def stochastic_gradient_descent(X, y, theta0_init, theta1_init, alpha, iterations):\n",
    "    \"\"\"\n",
    "    SGD: Update Œ∏ using only 1 random example per iteration\n",
    "    \"\"\"\n",
    "    theta0 = theta0_init\n",
    "    theta1 = theta1_init\n",
    "    m = len(X)\n",
    "    history = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # Choose random example\n",
    "        idx = np.random.randint(0, m)\n",
    "        x_i = X[idx]\n",
    "        y_i = y[idx]\n",
    "\n",
    "        # Prediction and error for this example\n",
    "        prediction = hypothesis(x_i, theta0, theta1)\n",
    "        error = prediction - y_i\n",
    "\n",
    "        # Update using only this example\n",
    "        theta0 = theta0 - alpha * error\n",
    "        theta1 = theta1 - alpha * error * x_i\n",
    "\n",
    "        # Calculate cost with all data (for monitoring)\n",
    "        cost = cost_function(X, y, theta0, theta1)\n",
    "        history.append((theta0, theta1, cost))\n",
    "\n",
    "    return theta0, theta1, history\n",
    "\n",
    "\n",
    "# Run SGD with normalized data\n",
    "theta0_sgd, theta1_sgd, history_sgd = stochastic_gradient_descent(\n",
    "    X_norm, y_norm, 0, 0, 0.1, 1000\n",
    ")\n",
    "\n",
    "print(\"Comparison Batch GD vs Stochastic GD (normalized data):\")\n",
    "print(f\"\\nBatch GD:\")\n",
    "print(f\"  Œ∏‚ÇÄ = {theta0_norm:.4f}, Œ∏‚ÇÅ = {theta1_norm:.4f}\")\n",
    "print(f\"  Final cost = {history_norm[-1][2]:.6f}\")\n",
    "\n",
    "print(f\"\\nStochastic GD:\")\n",
    "print(f\"  Œ∏‚ÇÄ = {theta0_sgd:.4f}, Œ∏‚ÇÅ = {theta1_sgd:.4f}\")\n",
    "print(f\"  Final cost = {history_sgd[-1][2]:.6f}\")\n",
    "\n",
    "# Visualize comparison\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "costs_batch_norm = [h[2] for h in history_norm]\n",
    "costs_sgd = [h[2] for h in history_sgd]\n",
    "plt.plot(costs_batch_norm, label='Batch GD', linewidth=2.5, color='blue')\n",
    "plt.plot(costs_sgd, label='Stochastic GD',\n",
    "         linewidth=2, alpha=0.8, color='orange')\n",
    "plt.xlabel('Iteration', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Cost J(Œ∏)', fontsize=12, fontweight='bold')\n",
    "plt.title('Batch GD vs Stochastic GD', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "theta0_norm_hist = [h[0] for h in history_norm]\n",
    "theta1_norm_hist = [h[1] for h in history_norm]\n",
    "theta0_sgd_hist = [h[0] for h in history_sgd]\n",
    "theta1_sgd_hist = [h[1] for h in history_sgd]\n",
    "plt.plot(theta0_norm_hist, theta1_norm_hist, 'o-', markersize=2, linewidth=1.5,\n",
    "         alpha=0.7, label='Batch GD', color='blue')\n",
    "plt.plot(theta0_sgd_hist, theta1_sgd_hist, 'o-', markersize=2, linewidth=1.5,\n",
    "         alpha=0.7, label='Stochastic GD', color='orange')\n",
    "plt.xlabel('Œ∏‚ÇÄ', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Œ∏‚ÇÅ', fontsize=12, fontweight='bold')\n",
    "plt.title('Trajectory: Batch vs Stochastic', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Observations:\")\n",
    "print(\"‚Ä¢ Batch GD: smooth and deterministic convergence\")\n",
    "print(\"‚Ä¢ Stochastic GD: more 'noise' but still converges!\")\n",
    "print(\"‚Ä¢ With NORMALIZED data, both work much better!\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ CODE EXECUTED SUCCESSFULLY!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüí° IMPORTANT LESSONS:\")\n",
    "print(\"1. Learning rate is VERY important!\")\n",
    "print(\"2. Data normalization = faster convergence\")\n",
    "print(\"3. With normalization, we can use much larger Œ±\")\n",
    "print(\"4. Without normalization: Œ± needs to be tiny (0.00001)\")\n",
    "print(\"5. With normalization: Œ± can be 0.1 (10,000x larger!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a10f32a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
