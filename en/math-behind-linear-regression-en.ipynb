{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d926547c",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "## Fundamental Concept\n",
    "\n",
    "Linear Regression is a machine learning algorithm whose main objective is to find the best line that passes through the available data points. This method seeks to establish a linear relationship between input and output variables, enabling accurate predictions on new data.\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "![Cover](../Assets/linear_regression/input.png)\n",
    "\n",
    "Linear Regression is classified as a **supervised learning** algorithm. This characterization is due to the fact that the algorithm uses a training dataset containing both input *features* and corresponding output *targets*. During the training process, the model learns to map the relationships between these known inputs and outputs.\n",
    "\n",
    "## Mathematical Model\n",
    "\n",
    "Once the model is trained, it becomes capable of predicting values $\\hat{y}$ (*y-hat*) for new inputs $x$. This prediction can be mathematically represented by the following function:\n",
    "\n",
    "![Cover](../Assets/linear_regression/w_e_b.png)\n",
    "\n",
    "$$f(x) = wx + b$$\n",
    "\n",
    "Where:\n",
    "- $f(x)$ or $\\hat{y}$ represents the predicted value\n",
    "- $w$ is the slope of the line\n",
    "- $b$ is the intercept of the line\n",
    "- $x$ is the input feature\n",
    "\n",
    "## Model Parameters\n",
    "\n",
    "The values chosen for parameters $w$ and $b$ are fundamental, as they completely determine the model's behavior. Specifically, these parameters define the prediction value $\\hat{y}_i$ for each example $i$, based on the corresponding input feature $x_i$. The optimization of these parameters during training is what enables the model to make accurate predictions.\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "To find the best values for $w$ and $b$, we reduce what we call the **cost function**. The goal is to choose values of $w$ and $b$ such that the prediction $\\hat{y}_i$ is as close as possible to the actual value $y_i$ for all examples in the training set.\n",
    "\n",
    "### Building the Cost Function\n",
    "\n",
    "The cost function works by comparing the actual value $y_i$ with the predicted value $\\hat{y}_i$. The difference between these values is expressed as:\n",
    "\n",
    "$$\\hat{y}_i - y_i$$\n",
    "\n",
    "This difference is called the prediction **error**.\n",
    "\n",
    "To obtain a more robust metric, we square this error. This serves two important purposes: eliminating negative values (ensuring that positive and negative errors don't cancel each other out) and penalizing larger errors more intensely:\n",
    "\n",
    "$$(\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "Since we want to quantify the error over the entire dataset, we sum the squared errors of all training examples:\n",
    "\n",
    "$$\\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2$$\n",
    "\n",
    "where $m$ represents the total number of training examples or datapoints.\n",
    "\n",
    "Finally, we calculate the average of these errors by dividing by $2m$. The choice of $2m$ instead of just $m$ is intended to simplify the mathematics later, especially when calculating the partial derivative of the cost function.\n",
    "\n",
    "Since $\\hat{y}_i$ can be represented as $f(x_i)$, the complete **cost function** is expressed as:\n",
    "\n",
    "$$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x_i) - y_i)^2$$\n",
    "\n",
    "This function, known as **Mean Squared Error** (MSE), is the most commonly used cost function in Linear Regression problems.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "To minimize the cost function, we use an algorithm called **Gradient Descent**. The process is relatively simple: we start with initial values for $w$ and $b$ (commonly $w = 0$ and $b = 0$).\n",
    "\n",
    "Next, we repeatedly update the parameters $w$ and $b$ in small steps with the goal of reducing the cost. We continue this iterative process until we reach the lowest possible cost (local minimum). When the algorithm reaches this point, we say it has **converged**.\n",
    "\n",
    "![Gradient Descent](../Assets/linear_regression/gradient.png)\n",
    "\n",
    "The process can be compared to descending a mountain: each step takes us closer to the bottom of the valley. The direction at each step is determined by the **gradient**, which always points in the direction of steepest ascent.\n",
    "\n",
    "![Gradient Descent Steps](../Assets/linear_regression/steps.png)\n",
    "\n",
    "To minimize cost, we move in the opposite direction of the gradient, that is, we take steps going downhill.\n",
    "\n",
    "### Gradient Descent Formula\n",
    "\n",
    "The parameter update at each iteration follows these equations:\n",
    "\n",
    "$w = w - \\alpha \\frac{\\partial J(w,b)}{\\partial w}$\n",
    "\n",
    "$b = b - \\alpha \\frac{\\partial J(w,b)}{\\partial b}$\n",
    "\n",
    "Where:\n",
    "- $w$ and $b$ are updated simultaneously at each iteration\n",
    "- $\\alpha$ is the **learning rate**\n",
    "- $\\frac{\\partial J(w,b)}{\\partial w}$ and $\\frac{\\partial J(w,b)}{\\partial b}$ are the partial derivatives of the cost function\n",
    "\n",
    "### Learning Rate\n",
    "\n",
    "Choosing a good value for $\\alpha$ is crucial:\n",
    "\n",
    "- **$\\alpha$ too small**: The algorithm will take very short steps, making convergence slow and time-consuming\n",
    "- **$\\alpha$ too large**: The algorithm may \"jump\" over the minimum, failing to converge or even diverging\n",
    "\n",
    "![Learning Rate](../Assets/linear_regression/alpha.png)\n",
    "\n",
    "## Calculating Partial Derivatives\n",
    "\n",
    "To implement Gradient Descent, we need to calculate the partial derivatives of the cost function $J(w,b)$ with respect to $w$ and $b$. We start by recalling our cost function:\n",
    "\n",
    "$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} (f(x_i) - y_i)^2$\n",
    "\n",
    "Since $f(x_i) = wx_i + b$, we can rewrite:\n",
    "\n",
    "$J(w, b) = \\frac{1}{2m} \\sum_{i=1}^{m} ((wx_i + b) - y_i)^2$\n",
    "\n",
    "### Partial Derivative with respect to $w$\n",
    "\n",
    "Applying the chain rule to derive $J(w,b)$ with respect to $w$:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{\\partial}{\\partial w} \\left[\\frac{1}{2m} \\sum_{i=1}^{m} ((wx_i + b) - y_i)^2\\right]$\n",
    "\n",
    "The constant $\\frac{1}{2m}$ can be taken out of the derivative:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{2m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial w} ((wx_i + b) - y_i)^2$\n",
    "\n",
    "Applying the chain rule: $\\frac{d}{dx}[g(x)]^2 = 2g(x) \\cdot g'(x)$\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{2m} \\sum_{i=1}^{m} 2((wx_i + b) - y_i) \\cdot \\frac{\\partial}{\\partial w}((wx_i + b) - y_i)$\n",
    "\n",
    "Since $\\frac{\\partial}{\\partial w}((wx_i + b) - y_i) = x_i$:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{2m} \\sum_{i=1}^{m} 2((wx_i + b) - y_i) \\cdot x_i$\n",
    "\n",
    "The factor $2$ cancels with the $2$ in the denominator:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} x_i \\big((w x_i + b) - y_i\\big)$\n",
    "\n",
    "Substituting $wx_i + b$ with $f(x_i)$:\n",
    "\n",
    "$\\boxed{\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} x_i(f(x_i) - y_i) }$\n",
    "\n",
    "### Partial Derivative with respect to $b$\n",
    "\n",
    "Following the same process for $b$:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{\\partial}{\\partial b} \\left[\\frac{1}{2m} \\sum_{i=1}^{m} ((wx_i + b) - y_i)^2\\right]$\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{2m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial b} ((wx_i + b) - y_i)^2$\n",
    "\n",
    "Applying the chain rule:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{2m} \\sum_{i=1}^{m} 2((wx_i + b) - y_i) \\cdot \\frac{\\partial}{\\partial b}((wx_i + b) - y_i)$\n",
    "\n",
    "Since $\\frac{\\partial}{\\partial b}((wx_i + b) - y_i) = 1$:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{2m} \\sum_{i=1}^{m} 2((wx_i + b) - y_i) \\cdot 1$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} ((wx_i + b) - y_i)$\n",
    "\n",
    "Substituting $(wx_i + b)$ with $f(x_i)$:\n",
    "\n",
    "$\\boxed{\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (f(x_i) - y_i)}$\n",
    "\n",
    "### Complete Algorithm\n",
    "\n",
    "With the derivatives calculated, the Gradient Descent algorithm for Linear Regression is:\n",
    "\n",
    "**Repeat until convergence:**\n",
    "\n",
    "$w = w - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} x_i(f(x_i) - y_i) $\n",
    "\n",
    "$b = b - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (f(x_i) - y_i)$\n",
    "\n",
    "Where $f(x_i) = wx_i + b$\n",
    "\n",
    "> **Important note**: Parameters $w$ and $b$ must be updated **simultaneously** at each iteration, that is, we calculate both derivatives with the old values before updating any parameter.\n",
    "\n",
    "# Below you will find an example using Python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6052bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af232dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing our Dataset\n",
    "training_set = pd.read_csv('../Datasets/Salary_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4433a524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of our Parameter and Target\n",
    "X_train = training_set['YearsExperience'].values\n",
    "y_train = training_set['Salary'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fa6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter visualization of the Parameter with the Target\n",
    "plt.scatter(X_train, y_train)\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e11051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need three main functions to implement linear regression:\n",
    "# 1) Cost function: Calculates how good the model is, using the Mean Squared Error (MSE), which measures the average squared error between the predicted values and the real values.\n",
    "\n",
    "# 2) Gradient function: Calculates the derivatives of the cost function with respect to the parameters w and b.\n",
    "\n",
    "# 3) Gradient descent function: Uses the gradients calculated by the gradient function to update the parameters w and b at each iteration, with the goal of minimizing the error.\n",
    "\n",
    "def cost_function(x, y, w, b):\n",
    "    m = len(x)\n",
    "    cost_sum = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f = w * x[i] + b\n",
    "        cost = (f - y[i]) ** 2\n",
    "        cost_sum += cost\n",
    "\n",
    "    total_cost = (1/(2*m)) * cost_sum\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def gradient_function(x, y, w, b):\n",
    "    m = len(x)\n",
    "    dc_dw = 0\n",
    "    dc_db = 0\n",
    "\n",
    "    for i in range(m):\n",
    "        f = w * x[i] + b\n",
    "\n",
    "        dc_dw += (f - y[i]) * x[i]\n",
    "        dc_db += (f - y[i])\n",
    "\n",
    "    dc_dw = (1/m) * dc_dw\n",
    "    dc_db = (1/m) * dc_db\n",
    "\n",
    "    return dc_dw, dc_db\n",
    "\n",
    "\n",
    "def gradient_descent(x, y, alpha, iterations):\n",
    "    w = 0\n",
    "    b = 0\n",
    "\n",
    "    for i in range(iterations):\n",
    "        dc_dw, dc_db = gradient_function(x, y, w, b)\n",
    "\n",
    "        w = w - alpha * dc_dw\n",
    "        b = b - alpha * dc_db\n",
    "\n",
    "        print(f\"Iteration {i}: Cost {cost_function(x, y, w, b)}\")\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf4294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicates the Learning Rate and the number of iterations\n",
    "learning_rate = 0.01\n",
    "iterations = 10000\n",
    "# Actually calculates the Gradient Descent\n",
    "final_w, final_b = gradient_descent(\n",
    "    X_train, y_train, learning_rate, iterations)\n",
    "print(f\"w: {final_w:.4f}, b: {final_b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db6a6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizes the Regression Line\n",
    "plt.scatter(X_train, y_train, label='Data Points')\n",
    "\n",
    "X_vals = np.linspace(min(X_train), max(X_train), 100)\n",
    "y_vals = final_w * X_vals + final_b\n",
    "plt.plot(X_vals, y_vals, color='red', label='Regression Line')\n",
    "\n",
    "plt.xlabel(\"Years of Experience\")\n",
    "plt.ylabel(\"Salary\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f69663",
   "metadata": {},
   "source": [
    "# Linear Regression Optimization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "After implementing the basic Linear Regression algorithm with Gradient Descent, two common challenges arise in practice:\n",
    "\n",
    "1. **Very high cost values** - Making interpretation and convergence difficult\n",
    "2. **Inadequate learning rate selection** - Resulting in slow convergence or training failure\n",
    "\n",
    "This guide presents two essential techniques to solve these problems: **feature normalization** and **systematic learning rate testing**.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Normalization\n",
    "\n",
    "### The Scale Problem\n",
    "\n",
    "When working with data on different scales, the Gradient Descent algorithm faces difficulties. For example, if one feature varies between 0 and 100 and another between 0 and 100,000, the gradients will have very different magnitudes, causing:\n",
    "\n",
    "- **Slow convergence**: The algorithm needs many iterations\n",
    "- **Numerical instability**: Extremely large cost values\n",
    "- **Difficulty choosing the learning rate**: An α that works for one feature may be inadequate for another\n",
    "\n",
    "### Solution: Z-Score Normalization\n",
    "\n",
    "**Z-score** normalization transforms the data so that it has mean 0 and standard deviation 1:\n",
    "\n",
    "$$X_{norm} = \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Where:\n",
    "- $\\mu$ is the mean of the data\n",
    "- $\\sigma$ is the standard deviation of the data\n",
    "\n",
    "### Implementation\n",
    "\n",
    "```python\n",
    "def normalize_features(X):\n",
    "    \"\"\"Normalizes data using z-score\"\"\"\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X_norm = (X - mean) / std\n",
    "    return X_norm\n",
    "```\n",
    "\n",
    "**Parameters:**\n",
    "- `X`: Array with original data\n",
    "\n",
    "**Returns:**\n",
    "- `X_norm`: Normalized data\n",
    "\n",
    "### Benefits of Normalization\n",
    "\n",
    "1. **Dramatic cost reduction**: From millions to values close to 0\n",
    "2. **Faster convergence**: Fewer iterations needed\n",
    "3. **Balanced gradients**: All features contribute equally\n",
    "4. **Facilitates learning rate selection**: Typical values (0.01 to 1.0) work well\n",
    "\n",
    "### Practical Example\n",
    "\n",
    "Before normalization:\n",
    "```\n",
    "X: min=1.10, max=10.50, mean=5.31\n",
    "y: min=$37,731, max=$122,391, mean=$76,003\n",
    "Initial cost: 1,344,612,525\n",
    "```\n",
    "\n",
    "After normalization:\n",
    "```\n",
    "X_norm: min=-1.51, max=1.86, mean=0.00\n",
    "y_norm: min=-1.42, max=1.72, mean=0.00\n",
    "Initial cost: 0.499\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Rate Optimization\n",
    "\n",
    "### The Learning Rate Dilemma\n",
    "\n",
    "The learning rate ($\\alpha$) controls the size of the steps the algorithm takes toward the minimum. Choosing this value is critical:\n",
    "\n",
    "| Learning Rate | Behavior | Result |\n",
    "|---------------|----------|--------|\n",
    "| **Too small** | Tiny steps | Very slow convergence |\n",
    "| **Adequate** | Balanced steps | Efficient convergence |\n",
    "| **Too large** | Excessive steps | Oscillation or divergence |\n",
    "\n",
    "### Systematic Testing Strategy\n",
    "\n",
    "Instead of choosing arbitrarily, we test several values and select the best based on performance metrics.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "```python\n",
    "def test_learning_rates(X, y):\n",
    "    \"\"\"Tests different learning rates to find the best one\"\"\"\n",
    "    learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    iterations = 5000\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTING DIFFERENT LEARNING RATES\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\n--- Testing α = {lr} ---\")\n",
    "        w, b, history = gradient_descent(\n",
    "            X, y, lr, iterations, print_every=1000)\n",
    "\n",
    "        # Calculate R²\n",
    "        predictions = w * X + b\n",
    "        ss_res = np.sum((y - predictions) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'final_cost': history[-1],\n",
    "            'r2': r2,\n",
    "            'w': w,\n",
    "            'b': b\n",
    "        })\n",
    "\n",
    "        print(f\"Final cost: {history[-1]:.6f}, R²: {r2:.4f}\")\n",
    "\n",
    "    # Select the best result\n",
    "    best = max(results, key=lambda x: x['r2'])\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"BEST LEARNING RATE: α = {best['lr']}\")\n",
    "    print(f\"R² = {best['r2']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return best['lr']\n",
    "```\n",
    "\n",
    "### Code Breakdown\n",
    "\n",
    "#### 1. Defining Candidates\n",
    "\n",
    "```python\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "```\n",
    "\n",
    "We test values on a **logarithmic scale**, covering from very conservative to aggressive values.\n",
    "\n",
    "#### 2. Training with Each Candidate\n",
    "\n",
    "```python\n",
    "for lr in learning_rates:\n",
    "    w, b, history = gradient_descent(X, y, lr, iterations, print_every=1000)\n",
    "```\n",
    "\n",
    "Each learning rate is tested with the same number of iterations for fair comparison.\n",
    "\n",
    "#### 3. R² Score Calculation\n",
    "\n",
    "The **coefficient of determination** ($R^2$) measures how well the model explains the data variability:\n",
    "\n",
    "$$R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "Where:\n",
    "- $SS_{res} = \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2$ (sum of squared residuals)\n",
    "- $SS_{tot} = \\sum_{i=1}^{m} (y_i - \\bar{y})^2$ (total sum of squares)\n",
    "\n",
    "```python\n",
    "predictions = w * X + b\n",
    "ss_res = np.sum((y - predictions) ** 2)\n",
    "ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "```\n",
    "\n",
    "#### R² Interpretation:\n",
    "\n",
    "- R² = 1.0: Perfect model (explains 100% of variance)\n",
    "- R² = 0.95: Excellent (explains 95% of variance)\n",
    "- R² = 0.70: Good (explains 70% of variance)\n",
    "- R² = 0.50: Average (explains 50% of variance)\n",
    "- R² < 0.30: Poor (model has little predictive power)\n",
    "- R² < 0: Model worse than simply using the mean\n",
    "\n",
    "#### 4. Storing Results\n",
    "\n",
    "```python\n",
    "results.append({\n",
    "    'lr': lr,\n",
    "    'final_cost': history[-1],\n",
    "    'r2': r2,\n",
    "    'w': w,\n",
    "    'b': b\n",
    "})\n",
    "```\n",
    "\n",
    "Each test is stored in a dictionary containing all relevant metrics.\n",
    "\n",
    "#### 5. Selecting the Best Learning Rate\n",
    "\n",
    "```python\n",
    "best = max(results, key=lambda x: x['r2'])\n",
    "```\n",
    "\n",
    "We use R² as the selection criterion, choosing the learning rate that maximizes this metric.\n",
    "\n",
    "# Code in Practice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032d879",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bonus Code - Normalization and Proper Learning Rate\n",
    "\n",
    "def normalize_features(X):\n",
    "    \"\"\"Normalizes data using z-score\"\"\"\n",
    "    mean = np.mean(X)\n",
    "    std = np.std(X)\n",
    "    X_norm = (X - mean) / std\n",
    "    return X_norm, mean, std\n",
    "\n",
    "\n",
    "def test_learning_rates(X, y):\n",
    "    \"\"\"Tests different learning rates to find the best one\"\"\"\n",
    "    learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    iterations = 5000\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(\"TESTING DIFFERENT LEARNING RATES\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\n--- Testing α = {lr} ---\")\n",
    "        w, b = gradient_descent(\n",
    "            X, y, lr, iterations)\n",
    "\n",
    "        # R²\n",
    "        predictions = w * X + b\n",
    "        ss_res = np.sum((y - predictions) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'r2': r2,\n",
    "            'w': w,\n",
    "            'b': b\n",
    "        })\n",
    "\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "    # Best result\n",
    "    best = max(results, key=lambda x: x['r2'])\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"BEST LEARNING RATE: α = {best['lr']}\")\n",
    "    print(f\"R² = {best['r2']:.4f}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return best['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e4597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_norm, x_mean, x_std = normalize_features(X_train)\n",
    "y_norm, y_mean, y_std = normalize_features(y_train)\n",
    "\n",
    "print(\"\\nNORMALIZED DATA\")\n",
    "print(\n",
    "    f\"X_norm: min={X_norm.min():.2f}, max={X_norm.max():.2f}, mean={X_norm.mean():.2f}\")\n",
    "print(\n",
    "    f\"y_norm: min={y_norm.min():.2f}, max={y_norm.max():.2f}, mean={y_norm.mean():.2f}\")\n",
    "\n",
    "# Test different learning rates\n",
    "best_lr = test_learning_rates(X_norm, y_norm)\n",
    "\n",
    "# Train again but now with the best learning rate\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"FINAL TRAINING WITH α = {best_lr}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "w_initial = 0\n",
    "b_initial = 0\n",
    "iterations = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d899722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Gradient Descent now with much lower cost (better)\n",
    "w_final, b_final = gradient_descent(\n",
    "    X_norm, y_norm, best_lr, iterations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20979985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize the Regression Line\n",
    "def plot_regression(X, y, w, b):\n",
    "    \"\"\"Plots data and regression line - VERY SIMPLE\"\"\"\n",
    "\n",
    "    # Calculate predictions\n",
    "    predictions = w * X + b\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(X, y, color='blue', s=100, alpha=0.6, label='Data')\n",
    "    plt.plot(X, predictions, color='red', linewidth=3, label='Regression')\n",
    "\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(f'Linear Regression: y = {w:.4f}x + {b:.4f}')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_regression(X_norm, y_norm, w_final, b_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c05311a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
