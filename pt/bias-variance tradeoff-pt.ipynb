{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f114e0",
   "metadata": {},
   "source": [
    "# Bias-Variance Trade-off, Overfitting e Underfitting\n",
    "\n",
    "![Capa](../Assets/bias_variance_tradeoff/capa3.png)\n",
    "\n",
    "## 1. O Problema Fundamental\n",
    "\n",
    "Quando treinamos um modelo de Machine Learning, queremos que ele funcione bem em dados novos (que ele nunca viu). Mas existem dois tipos de erro que podem acontecer:\n",
    "\n",
    "1. Erro no Training Set (dados de treino)\n",
    "2. Erro no Test Set (dados novos)\n",
    "\n",
    "O modelo ideal tem baixo erro em ambos. Mas na prática, existe um trade-off entre dois conceitos: Bias e Variance.\n",
    "\n",
    "## 2. O que é Bias (Viés)?\n",
    "\n",
    "Bias é o erro causado por suposições erradas ou modelo muito simples.\n",
    "\n",
    "### Analogia Prática\n",
    "\n",
    "Imagine que você está tentando acertar um alvo com flechas:\n",
    "- Alto Bias = Suas flechas consistentemente acertam longe do centro (você está mirando errado)\n",
    "- Baixo Bias = Suas flechas, em média, acertam perto do centro\n",
    "\n",
    "### No Machine Learning\n",
    "\n",
    "Um modelo com alto bias:\n",
    "- É muito simples para capturar os padrões dos dados\n",
    "- Faz suposições fortes sobre a relação entre X e y\n",
    "- Resulta em Underfitting (subajuste)\n",
    "\n",
    "Exemplo: Usar uma reta para modelar dados com curva\n",
    "\n",
    "![High Bias](../Assets/bias_variance_tradeoff/1.jpg)\n",
    "\n",
    "```\n",
    "Modelo muito simples!\n",
    "Não captura o padrão real.\n",
    "```\n",
    "\n",
    "## 3. O que é Variance (Variância)?\n",
    "\n",
    "Variance é o erro causado por sensibilidade excessiva aos dados de treino.\n",
    "\n",
    "### Analogia Prática\n",
    "\n",
    "Continuando com as flechas:\n",
    "- Alta Variance = Suas flechas estão espalhadas por todo lado (inconsistente)\n",
    "- Baixa Variance = Suas flechas estão agrupadas próximas umas das outras (consistente)\n",
    "\n",
    "### No Machine Learning\n",
    "\n",
    "Um modelo com alta variance:\n",
    "- É muito complexo e \"decora\" os dados de treino\n",
    "- Se adapta demais ao ruído dos dados\n",
    "- Resulta em Overfitting (sobreajuste)\n",
    "\n",
    "Exemplo: Usar polinômio de grau 10 para modelar dados simples\n",
    "\n",
    "![High Variance](../Assets/bias_variance_tradeoff/2.png)\n",
    "\n",
    "``` \n",
    "Modelo muito complexo!\n",
    "Passa por todos os pontos mas \n",
    "não generaliza para dados novos.\n",
    "```\n",
    "\n",
    "## 4. Underfitting (Subajuste) - Alto Bias\n",
    "\n",
    "### O que é?\n",
    "\n",
    "Underfitting acontece quando o modelo é simples demais para capturar os padrões dos dados.\n",
    "\n",
    "### Características\n",
    "\n",
    "- Alto erro no training set\n",
    "- Alto erro no test set\n",
    "- Modelo não aprendeu o padrão básico dos dados\n",
    "\n",
    "### Exemplo Numérico\n",
    "\n",
    "Dataset com relação quadrática: $y = x^2 + \\text{ruído}$\n",
    "\n",
    "| x  | y (real) |\n",
    "|----|----------|\n",
    "| 1  | 1.2      |\n",
    "| 2  | 4.1      |\n",
    "| 3  | 9.3      |\n",
    "| 4  | 16.2     |\n",
    "| 5  | 25.1     |\n",
    "\n",
    "Modelo 1: Reta $h(x) = \\theta_0 + \\theta_1 x$\n",
    "\n",
    "Resultado:\n",
    "- Training Error: 45.2\n",
    "- Test Error: 47.8\n",
    "\n",
    "Por quê? Uma reta não consegue capturar a curvatura dos dados!\n",
    "\n",
    "### Como Identificar Underfitting?\n",
    "\n",
    "1. Training error alto (aproximadamente 40-50% de erro)\n",
    "2. Test error similar ao training (diferença pequena)\n",
    "3. Curva de aprendizado: ambos os erros ficam altos mesmo com mais dados\n",
    "\n",
    "## 5. Overfitting (Sobreajuste) - Alta Variance\n",
    "\n",
    "### O que é?\n",
    "\n",
    "Overfitting acontece quando o modelo é complexo demais e \"decora\" os dados de treino, incluindo o ruído.\n",
    "\n",
    "### Características\n",
    "\n",
    "- Baixo erro no training set\n",
    "- Alto erro no test set\n",
    "- Modelo decorou em vez de aprender\n",
    "\n",
    "### Exemplo Numérico\n",
    "\n",
    "Dataset com relação quadrática: $y = x^2 + \\text{ruído}$\n",
    "\n",
    "| x  | y (treino) | y (test) |\n",
    "|----|------------|----------|\n",
    "| 1  | 1.2        | 0.9      |\n",
    "| 2  | 4.1        | 3.8      |\n",
    "| 3  | 9.3        | 9.5      |\n",
    "| 4  | 16.2       | 15.7     |\n",
    "| 5  | 25.1       | 25.4     |\n",
    "\n",
    "Modelo 2: Polinômio grau 10 $h(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2 + ... + \\theta_{10} x^{10}$\n",
    "\n",
    "Resultado:\n",
    "- Training Error: 0.01 (praticamente zero!)\n",
    "- Test Error: 152.7 (explodiu!)\n",
    "\n",
    "Por quê? O modelo se ajustou perfeitamente aos dados de treino (incluindo ruído), mas não generaliza para dados novos.\n",
    "\n",
    "### Como Identificar Overfitting?\n",
    "\n",
    "1. Training error muito baixo (aproximadamente 1-5% de erro)\n",
    "2. Test error muito alto (10x maior que training)\n",
    "3. Gap grande entre training e test error\n",
    "4. Curva de aprendizado: training continua caindo, test começa a subir\n",
    "\n",
    "## 6. O Modelo Ideal - Just Right (Goldilocks)\n",
    "\n",
    "### Características\n",
    "\n",
    "- Baixo erro no training set\n",
    "- Baixo erro no test set\n",
    "- Gap pequeno entre os dois\n",
    "\n",
    "### Exemplo Numérico\n",
    "\n",
    "Modelo 3: Polinômio grau 2 $h(x) = \\theta_0 + \\theta_1 x + \\theta_2 x^2$\n",
    "\n",
    "Resultado:\n",
    "- Training Error: 2.1\n",
    "- Test Error: 2.8\n",
    "- Gap: apenas 0.7\n",
    "\n",
    "Perfeito! Captura o padrão real (quadrático) sem memorizar o ruído.\n",
    "\n",
    "## 7. Bias-Variance Trade-off\n",
    "\n",
    "### A Equação do Erro Total\n",
    "\n",
    "$$\\text{Erro Total} = \\text{Bias}^2 + \\text{Variance} + \\text{Ruído Irredutível}$$\n",
    "\n",
    "Onde:\n",
    "- Bias²: erro por modelo muito simples\n",
    "- Variance: erro por modelo muito sensível\n",
    "- Ruído irredutível: erro inerente aos dados (não dá pra evitar)\n",
    "\n",
    "### O Trade-off\n",
    "\n",
    "![Bias-Variance Trade-off](../Assets/bias_variance_tradeoff/3.png)\n",
    "\n",
    "### Relação Inversa\n",
    "\n",
    "Aumentar complexidade:\n",
    "- Bias diminui (captura padrões complexos)\n",
    "- Variance aumenta (sensível ao ruído)\n",
    "  \n",
    "Diminuir complexidade:\n",
    "- Bias aumenta (não captura padrões)\n",
    "- Variance diminui (mais estável)\n",
    "\n",
    "## 8. Como Diagnosticar o Problema?\n",
    "\n",
    "### Comparar Erros\n",
    "\n",
    "| Situação | Training Error | Test Error | Gap | Diagnóstico |\n",
    "|----------|---------------|------------|-----|-------------|\n",
    "| A        | 45%           | 47%        | 2%  | Underfitting (alto bias) |\n",
    "| B        | 2%            | 3%         | 1%  | Just Right |\n",
    "| C        | 1%            | 25%        | 24% | Overfitting (alta variance) |\n",
    "\n",
    "## 9. Como Corrigir Underfitting (Alto Bias)?\n",
    "\n",
    "### Soluções\n",
    "\n",
    "### 1. Aumentar a Complexidade do Modelo\n",
    "\n",
    "Antes:\n",
    "```python\n",
    "# Modelo muito simples\n",
    "h(x) = θ₀ + θ₁x  # Reta\n",
    "```\n",
    "\n",
    "Depois:\n",
    "```python\n",
    "# Modelo mais complexo\n",
    "h(x) = θ₀ + θ₁x + θ₂x²  # Parábola\n",
    "```\n",
    "\n",
    "### 2. Adicionar Mais Features\n",
    "\n",
    "Antes:\n",
    "```python\n",
    "# Apenas 1 feature\n",
    "X = [size]\n",
    "```\n",
    "\n",
    "Depois:\n",
    "```python\n",
    "# Múltiplas features\n",
    "X = [size, bedrooms, age, location]\n",
    "```\n",
    "\n",
    "### 3. Feature Engineering\n",
    "\n",
    "Criar features derivadas:\n",
    "```python\n",
    "# Features originais\n",
    "x₁ = size\n",
    "\n",
    "# Features derivadas\n",
    "x₂ = size²\n",
    "x₃ = size³\n",
    "x₄ = sqrt(size)\n",
    "```\n",
    "\n",
    "### 4. Remover Regularização\n",
    "\n",
    "Se você está usando regularização (λ), diminua ou remova:\n",
    "```python\n",
    "# Antes: λ muito alto\n",
    "λ = 10  # Força modelo simples\n",
    "\n",
    "# Depois: λ menor ou zero\n",
    "λ = 0  # Permite modelo mais flexível\n",
    "```\n",
    "\n",
    "### 5. Treinar por Mais Tempo\n",
    "\n",
    "Para redes neurais, aumente epochs:\n",
    "```python\n",
    "# Antes\n",
    "epochs = 10  # Parou cedo\n",
    "\n",
    "# Depois\n",
    "epochs = 100  # Treinou mais\n",
    "```\n",
    "\n",
    "### Cuidado!\n",
    "\n",
    "Ao corrigir underfitting, você pode causar overfitting. Monitore sempre o test error!\n",
    "\n",
    "## 10. Como Corrigir Overfitting (Alta Variance)?\n",
    "\n",
    "### Soluções\n",
    "\n",
    "### 1. Coletar Mais Dados\n",
    "\n",
    "A melhor solução! Mais dados ajudam o modelo a generalizar melhor.\n",
    "\n",
    "Antes:\n",
    "```python\n",
    "m = 100  # Poucos exemplos\n",
    "```\n",
    "\n",
    "Depois:\n",
    "```python\n",
    "m = 10000  # Muitos exemplos\n",
    "```\n",
    "\n",
    "Por quê funciona? Com mais dados, o modelo não consegue \"decorar\" tudo, forçando-o a aprender padrões reais.\n",
    "\n",
    "### 2. Reduzir Complexidade do Modelo\n",
    "\n",
    "Antes:\n",
    "```python\n",
    "# Polinômio grau 10\n",
    "h(x) = θ₀ + θ₁x + θ₂x² + ... + θ₁₀x¹⁰\n",
    "```\n",
    "\n",
    "Depois:\n",
    "```python\n",
    "# Polinômio grau 2\n",
    "h(x) = θ₀ + θ₁x + θ₂x²\n",
    "```\n",
    "\n",
    "### 3. Regularização (L1 ou L2)\n",
    "\n",
    "Adicionar penalidade aos pesos grandes:\n",
    "\n",
    "L2 Regularization (Ridge):\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} \\theta_j^2$$\n",
    "\n",
    "```python\n",
    "# λ controla o quanto penalizamos\n",
    "λ = 0.1   # Regularização moderada\n",
    "λ = 1.0   # Regularização forte\n",
    "λ = 10.0  # Regularização muito forte\n",
    "```\n",
    "\n",
    "L1 Regularization (Lasso):\n",
    "$$J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2 + \\lambda \\sum_{j=1}^{n} |\\theta_j|$$\n",
    "\n",
    "Efeito: Força os pesos $\\theta$ a serem pequenos, tornando o modelo mais simples.\n",
    "\n",
    "### 4. Feature Selection (Remover Features)\n",
    "\n",
    "Antes:\n",
    "```python\n",
    "# Muitas features (20)\n",
    "X = [size, bedrooms, age, location, ..., feature_20]\n",
    "```\n",
    "\n",
    "Depois:\n",
    "```python\n",
    "# Apenas features importantes (5)\n",
    "X = [size, bedrooms, location, age, bathrooms]\n",
    "```\n",
    "\n",
    "### 5. Cross-Validation\n",
    "\n",
    "Dividir dados em k-folds para validação:\n",
    "\n",
    "![k-folds](../Assets/bias_variance_tradeoff/4.png)\n",
    "\n",
    "## 11. Resumo - Tabela de Decisão\n",
    "\n",
    "| Problema | Sintomas | Soluções |\n",
    "|----------|----------|----------|\n",
    "| Underfitting | Training error alto<br>Test error alto<br>Gap pequeno | 1. Aumentar complexidade do modelo<br>2. Adicionar mais features<br>3. Feature engineering<br>4. Diminuir regularização (λ)<br>5. Treinar mais tempo |\n",
    "| Overfitting | Training error baixo<br>Test error alto<br>Gap grande | 1. Coletar mais dados<br>2. Reduzir complexidade do modelo<br>3. Adicionar regularização (L1/L2)<br>4. Remover features<br>5. Cross-validation |\n",
    "| Just Right | Training error baixo<br>Test error baixo<br>Gap pequeno | Continue assim! |\n",
    "\n",
    "## 12. Exemplo Prático Completo\n",
    "\n",
    "### Dataset: Prever preço de casas\n",
    "\n",
    "```python\n",
    "# Dados\n",
    "X_train: 80 casas\n",
    "y_train: preços\n",
    "\n",
    "X_test: 20 casas\n",
    "y_test: preços\n",
    "```\n",
    "\n",
    "### Tentativa 1: Reta Simples\n",
    "\n",
    "```python\n",
    "modelo = LinearRegression()  # h(x) = θ₀ + θ₁x\n",
    "```\n",
    "\n",
    "Resultado:\n",
    "- Training Error: 42%\n",
    "- Test Error: 45%\n",
    "- Diagnóstico: UNDERFITTING\n",
    "\n",
    "Ação: Aumentar complexidade\n",
    "\n",
    "### Tentativa 2: Polinômio Grau 2\n",
    "\n",
    "```python\n",
    "modelo = PolynomialRegression(degree=2)  # h(x) = θ₀ + θ₁x + θ₂x²\n",
    "```\n",
    "\n",
    "Resultado:\n",
    "- Training Error: 5%\n",
    "- Test Error: 8%\n",
    "- Diagnóstico: JUST RIGHT\n",
    "\n",
    "Ação: Sucesso! Modelo balanceado.\n",
    "\n",
    "### Tentativa 3: Polinômio Grau 10\n",
    "\n",
    "```python\n",
    "modelo = PolynomialRegression(degree=10)\n",
    "```\n",
    "\n",
    "Resultado:\n",
    "- Training Error: 0.5%\n",
    "- Test Error: 45%\n",
    "- Diagnóstico: OVERFITTING\n",
    "\n",
    "Ação: Aplicar regularização\n",
    "\n",
    "### Tentativa 4: Polinômio Grau 10 + Regularização\n",
    "\n",
    "```python\n",
    "modelo = Ridge(degree=10, alpha=1.0)  # α = λ (regularização)\n",
    "```\n",
    "\n",
    "Resultado:\n",
    "- Training Error: 4%\n",
    "- Test Error: 6%\n",
    "- Diagnóstico: JUST RIGHT\n",
    "\n",
    "Ação: Sucesso! Regularização resolveu.\n",
    "\n",
    "## 13. Dicas Finais\n",
    "\n",
    "### Boas Práticas\n",
    "\n",
    "1. Sempre separe train/test (80/20 ou 70/30)\n",
    "2. Use cross-validation para escolher hiperparâmetros\n",
    "3. Comece simples, adicione complexidade gradualmente\n",
    "4. Monitore ambos os erros (training e test)\n",
    "5. Plote curvas de aprendizado para visualizar\n",
    "\n",
    "### Erros Comuns\n",
    "\n",
    "1. Não separar test set (treinar e testar nos mesmos dados)\n",
    "2. Usar test set para ajustar modelo (data leakage)\n",
    "3. Complexidade excessiva desde o início\n",
    "4. Ignorar o training error (focar só no test)\n",
    "5. Não usar regularização quando apropriado\n",
    "\n",
    "---\n",
    "\n",
    "Lembre-se:\n",
    "_\"O melhor modelo não é o que melhor se ajusta aos dados de treino, mas sim o que melhor generaliza para dados novos.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e21c1d2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
