{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "339aba01",
   "metadata": {},
   "source": [
    "# Entendendo Entropia em Ci√™ncia de Dados\n",
    "\n",
    "## Introdu√ß√£o\n",
    "\n",
    "A entropia √© um conceito fundamental em ci√™ncia de dados, usado em diversos algoritmos e t√©cnicas. Ela aparece em:\n",
    "\n",
    "- **√Årvores de Classifica√ß√£o**: Para decidir como dividir os dados\n",
    "- **Informa√ß√£o M√∫tua**: Para quantificar rela√ß√µes entre vari√°veis\n",
    "- **Entropia Relativa** (dist√¢ncia de Kullback-Leibler): Para comparar distribui√ß√µes\n",
    "- **Entropia Cruzada**: Usada em redes neurais e algoritmos de redu√ß√£o de dimensionalidade como t-SNE e UMAP\n",
    "\n",
    "O que todas essas aplica√ß√µes t√™m em comum √© o uso da entropia para **quantificar similaridades e diferen√ßas**.\n",
    "\n",
    "## O Conceito de Surpresa\n",
    "\n",
    "Antes de entender entropia, precisamos entender **surpresa**.\n",
    "\n",
    "### Exemplo das Galinhas\n",
    "\n",
    "Imagine tr√™s √°reas com galinhas laranjas üü† e azuis üîµ:\n",
    "\n",
    "**√Årea A**: 6 galinhas laranjas e 1 galinha azul\n",
    "\n",
    "- Probabilidade de pegar laranja: $P(\\text{laranja}) = \\frac{6}{7} \\approx 0.86$\n",
    "- Probabilidade de pegar azul: $P(\\text{azul}) = \\frac{1}{7} \\approx 0.14$\n",
    "- **N√£o seria muito surpreendente** pegar uma galinha laranja\n",
    "- **Seria relativamente surpreendente** pegar uma galinha azul\n",
    "\n",
    "**√Årea B**: 1 galinha laranja e 10 galinhas azuis\n",
    "\n",
    "- Probabilidade de pegar azul: $P(\\text{azul}) = \\frac{10}{11} \\approx 0.91$\n",
    "- Probabilidade de pegar laranja: $P(\\text{laranja}) = \\frac{1}{11} \\approx 0.09$\n",
    "- **N√£o seria muito surpreendente** pegar uma galinha azul\n",
    "- **Seria relativamente surpreendente** pegar uma galinha laranja\n",
    "\n",
    "**√Årea C**: 5 galinhas laranjas e 5 galinhas azuis\n",
    "\n",
    "- Probabilidade de pegar qualquer cor: $P = \\frac{5}{10} = 0.5$\n",
    "- Ficamos **igualmente surpresos** independente da cor\n",
    "\n",
    "### Rela√ß√£o Inversa\n",
    "\n",
    "A surpresa tem uma **rela√ß√£o inversa** com a probabilidade:\n",
    "\n",
    "- **Probabilidade baixa** ‚Üí **Surpresa alta**\n",
    "- **Probabilidade alta** ‚Üí **Surpresa baixa**\n",
    "\n",
    "## Calculando a Surpresa\n",
    "\n",
    "### Por que n√£o usar apenas o inverso da probabilidade?\n",
    "\n",
    "Poder√≠amos pensar em usar simplesmente:\n",
    "\n",
    "$$\\text{Surpresa} = \\frac{1}{P}$$\n",
    "\n",
    "Mas isso tem um problema. Considere uma moeda que **sempre** d√° cara:\n",
    "\n",
    "- $P(\\text{cara}) = 1$\n",
    "- $\\text{Surpresa} = \\frac{1}{1} = 1$\n",
    "\n",
    "Mas se a moeda sempre d√° cara, n√£o deveria haver **nenhuma surpresa** (deveria ser zero)!\n",
    "\n",
    "### A F√≥rmula Correta da Surpresa\n",
    "\n",
    "Usamos o **logaritmo do inverso da probabilidade**:\n",
    "\n",
    "$$\\text{Surpresa} = \\log\\left(\\frac{1}{P}\\right) = -\\log(P)$$\n",
    "\n",
    "Com essa f√≥rmula:\n",
    "\n",
    "- Quando $P = 1$ (certeza): $\\text{Surpresa} = -\\log(1) = 0$ ‚úì\n",
    "- Quando $P \\to 0$ (evento raro): $\\text{Surpresa} \\to \\infty$ ‚úì\n",
    "\n",
    "**Nota importante**: Para problemas bin√°rios (dois resultados poss√≠veis), usamos $\\log_2$ (logaritmo base 2).\n",
    "\n",
    "### Exemplo: Moeda Viciada\n",
    "\n",
    "Considere uma moeda que d√° cara 90% do tempo e coroa 10% do tempo:\n",
    "\n",
    "$$\\text{Surpresa(cara)} = -\\log_2(0.9) = 0.15$$\n",
    "\n",
    "$$\\text{Surpresa(coroa)} = -\\log_2(0.1) = 3.32$$\n",
    "\n",
    "Como esperado, obter coroa √© **muito mais surpreendente** que obter cara.\n",
    "\n",
    "## Da Surpresa para a Entropia\n",
    "\n",
    "### Surpresa Total de uma Sequ√™ncia\n",
    "\n",
    "Se jogarmos a moeda 3 vezes e obtivermos: **cara, cara, coroa**\n",
    "\n",
    "A probabilidade dessa sequ√™ncia √©:\n",
    "\n",
    "$$P = 0.9 \\times 0.9 \\times 0.1 = 0.081$$\n",
    "\n",
    "A surpresa total √©:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\text{Surpresa total} &= -\\log_2(0.081)\\\\\n",
    "&= -\\log_2(0.9 \\times 0.9 \\times 0.1)\\\\\n",
    "&= -\\log_2(0.9) - \\log_2(0.9) - \\log_2(0.1)\\\\\n",
    "&= 0.15 + 0.15 + 3.32 = 3.62\n",
    "\\end{align}$$\n",
    "\n",
    "A surpresa de uma sequ√™ncia √© a **soma das surpresas individuais**!\n",
    "\n",
    "### Estimando Surpresa para M√∫ltiplos Lan√ßamentos\n",
    "\n",
    "Para 100 lan√ßamentos da moeda:\n",
    "\n",
    "| Resultado | Probabilidade | Surpresa | Ocorr√™ncias Esperadas | Surpresa Total Esperada |\n",
    "|-----------|---------------|----------|----------------------|------------------------|\n",
    "| Cara | 0.9 | 0.15 | $0.9 \\times 100 = 90$ | $90 \\times 0.15 = 13.5$ |\n",
    "| Coroa | 0.1 | 3.32 | $0.1 \\times 100 = 10$ | $10 \\times 3.32 = 33.2$ |\n",
    "\n",
    "Surpresa total esperada: $13.5 + 33.2 = 46.7$\n",
    "\n",
    "### Defini√ß√£o de Entropia\n",
    "\n",
    "A **entropia** √© a **surpresa m√©dia por lan√ßamento**:\n",
    "\n",
    "$$H = \\frac{\\text{Surpresa Total}}{100} = \\frac{46.7}{100} = 0.47$$\n",
    "\n",
    "Ou seja, **entropia √© a surpresa esperada cada vez que realizamos o experimento**.\n",
    "\n",
    "## A F√≥rmula da Entropia\n",
    "\n",
    "### Forma Geral\n",
    "\n",
    "Como os termos \"100\" se cancelam:\n",
    "\n",
    "$$H = P(\\text{cara}) \\times \\text{Surpresa(cara)} + P(\\text{coroa}) \\times \\text{Surpresa(coroa)}$$\n",
    "\n",
    "$$H = 0.9 \\times 0.15 + 0.1 \\times 3.32 = 0.47$$\n",
    "\n",
    "### Nota√ß√£o com Somat√≥rio\n",
    "\n",
    "$$H = \\sum_{i} P(x_i) \\times \\text{Surpresa}(x_i)$$\n",
    "\n",
    "Onde $x_i$ representa cada resultado poss√≠vel.\n",
    "\n",
    "### Substituindo a F√≥rmula da Surpresa\n",
    "\n",
    "Substituindo $\\text{Surpresa}(x_i) = -\\log_2(P(x_i))$:\n",
    "\n",
    "$$H = \\sum_{i} P(x_i) \\times \\left(-\\log_2(P(x_i))\\right)$$\n",
    "\n",
    "$$H = -\\sum_{i} P(x_i) \\log_2(P(x_i))$$\n",
    "\n",
    "Esta √© a **f√≥rmula cl√°ssica da entropia de Shannon** (1948)!\n",
    "\n",
    "### Forma Alternativa (Mais Intuitiva)\n",
    "\n",
    "Tamb√©m podemos escrever:\n",
    "\n",
    "$$H = \\sum_{i} P(x_i) \\log_2\\left(\\frac{1}{P(x_i)}\\right)$$\n",
    "\n",
    "Esta forma deixa mais claro que estamos calculando a **m√©dia ponderada da surpresa**.\n",
    "\n",
    "## Calculando Entropia nos Exemplos\n",
    "\n",
    "### √Årea A: 6 laranjas, 1 azul\n",
    "\n",
    "$$\\begin{align}\n",
    "H_A &= \\frac{6}{7} \\log_2\\left(\\frac{7}{6}\\right) + \\frac{1}{7} \\log_2\\left(\\frac{7}{1}\\right)\\\\\n",
    "&= \\frac{6}{7} \\times 0.22 + \\frac{1}{7} \\times 2.81\\\\\n",
    "&= 0.19 + 0.40\\\\\n",
    "&= 0.59\n",
    "\\end{align}$$\n",
    "\n",
    "### √Årea B: 1 laranja, 10 azuis\n",
    "\n",
    "$$\\begin{align}\n",
    "H_B &= \\frac{1}{11} \\log_2\\left(\\frac{11}{1}\\right) + \\frac{10}{11} \\log_2\\left(\\frac{11}{10}\\right)\\\\\n",
    "&= \\frac{1}{11} \\times 3.46 + \\frac{10}{11} \\times 0.14\\\\\n",
    "&= 0.31 + 0.13\\\\\n",
    "&= 0.44\n",
    "\\end{align}$$\n",
    "\n",
    "### √Årea C: 5 laranjas, 5 azuis\n",
    "\n",
    "$$\\begin{align}\n",
    "H_C &= \\frac{5}{10} \\log_2\\left(\\frac{10}{5}\\right) + \\frac{5}{10} \\log_2\\left(\\frac{10}{5}\\right)\\\\\n",
    "&= 0.5 \\times 1 + 0.5 \\times 1\\\\\n",
    "&= 1.0\n",
    "\\end{align}$$\n",
    "\n",
    "## Interpreta√ß√£o dos Resultados\n",
    "\n",
    "### Comparando as Entropias\n",
    "\n",
    "- **√Årea C**: $H = 1.0$ (maior entropia)\n",
    "- **√Årea A**: $H = 0.59$ (entropia intermedi√°ria)\n",
    "- **√Årea B**: $H = 0.44$ (menor entropia)\n",
    "\n",
    "### O que isso significa?\n",
    "\n",
    "A entropia √© **m√°xima** quando temos **igualdade** entre as categorias (√Årea C: 50%-50%).\n",
    "\n",
    "A entropia **diminui** √† medida que aumentamos a **diferen√ßa** entre as quantidades (√Åreas A e B s√£o mais desbalanceadas).\n",
    "\n",
    "### Entropia como Medida de Similaridade\n",
    "\n",
    "Podemos usar entropia para quantificar:\n",
    "\n",
    "- **Alta entropia** ‚Üí Alta incerteza ‚Üí Distribui√ß√£o uniforme ‚Üí Mais \"mistura\"\n",
    "- **Baixa entropia** ‚Üí Baixa incerteza ‚Üí Distribui√ß√£o concentrada ‚Üí Menos \"mistura\"\n",
    "\n",
    "## Propriedades Importantes\n",
    "\n",
    "### 1. Entropia √© sempre n√£o-negativa\n",
    "\n",
    "$$H \\geq 0$$\n",
    "\n",
    "### 2. Entropia m√°xima para distribui√ß√£o uniforme\n",
    "\n",
    "Para $n$ categorias igualmente prov√°veis:\n",
    "\n",
    "$$H_{\\text{m√°x}} = \\log_2(n)$$\n",
    "\n",
    "Exemplo: Com 2 categorias (bin√°rio): $H_{\\text{m√°x}} = \\log_2(2) = 1$\n",
    "\n",
    "### 3. Entropia m√≠nima quando h√° certeza\n",
    "\n",
    "Se uma categoria tem probabilidade 1 e todas as outras t√™m probabilidade 0:\n",
    "\n",
    "$$H_{\\text{m√≠n}} = 0$$\n",
    "\n",
    "## Aplica√ß√µes Pr√°ticas\n",
    "\n",
    "### 1. √Årvores de Decis√£o\n",
    "\n",
    "Usamos entropia para decidir qual atributo usar para dividir os dados. Escolhemos o atributo que **maximiza a redu√ß√£o da entropia** (ganho de informa√ß√£o).\n",
    "\n",
    "### 2. Informa√ß√£o M√∫tua\n",
    "\n",
    "Mede quanto conhecer uma vari√°vel reduz a incerteza sobre outra:\n",
    "\n",
    "$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$\n",
    "\n",
    "### 3. Entropia Cruzada (Cross-Entropy)\n",
    "\n",
    "Usada como fun√ß√£o de perda em classifica√ß√£o:\n",
    "\n",
    "$$H(p,q) = -\\sum_{i} p(x_i) \\log(q(x_i))$$\n",
    "\n",
    "Onde $p$ √© a distribui√ß√£o real e $q$ √© a distribui√ß√£o prevista.\n",
    "\n",
    "## Resumo\n",
    "\n",
    "1. **Surpresa** quantifica o qu√£o inesperado √© um evento: $-\\log(P)$\n",
    "\n",
    "2. **Entropia** √© a **surpresa m√©dia esperada**: \n",
    "   $$H = -\\sum_{i} P(x_i) \\log(P(x_i))$$\n",
    "\n",
    "3. **Entropia m√°xima** ocorre quando todos os resultados s√£o **igualmente prov√°veis**\n",
    "\n",
    "4. **Entropia m√≠nima** (zero) ocorre quando h√° **certeza total** sobre o resultado\n",
    "\n",
    "5. Entropia √© fundamental para **medir incerteza** e **quantificar informa√ß√£o** em ci√™ncia de dados\n",
    "\n",
    "---\n",
    "\n",
    "*\"Da pr√≥xima vez que voc√™ quiser surpreender algu√©m, apenas sussurre: o logaritmo do inverso da probabilidade!\"* üé≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2236f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
